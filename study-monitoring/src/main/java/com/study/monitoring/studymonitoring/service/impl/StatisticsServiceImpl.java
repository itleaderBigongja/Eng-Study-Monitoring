package com.study.monitoring.studymonitoring.service.impl;

import com.study.monitoring.studymonitoring.converter.*;
import com.study.monitoring.studymonitoring.mapper.StatisticsMapper;
import com.study.monitoring.studymonitoring.model.dto.request.*;
import com.study.monitoring.studymonitoring.model.dto.response.*;
import com.study.monitoring.studymonitoring.model.vo.StatisticsVO;
import com.study.monitoring.studymonitoring.service.ElasticsearchService;
import com.study.monitoring.studymonitoring.service.PrometheusService;
import com.study.monitoring.studymonitoring.service.StatisticsService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.stream.Collectors;

@Slf4j
@Service
@RequiredArgsConstructor
public class StatisticsServiceImpl implements StatisticsService {

    private final PrometheusService prometheusService;
    private final ElasticsearchService elasticsearchService;
    private final StatisticsMapper statisticsMapper;
    private final PrometheusStatisticsConverter prometheusStatisticsConverter;
    private final LogsConverter logsConverter;
    private final AccessLogsConverter accessLogsConverter;

    @Value("${monitoring.retention.prometheus-days}")
    private int prometheusDays;  // default: 30

    private static final DateTimeFormatter DATE_FORMATTER = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");

    /**
     * ì‹œê³„ì—´ ë°ì´í„° í†µê³„ ì¡°íšŒ (PostgreSQL + Prometheus í•˜ì´ë¸Œë¦¬ë“œ ì¡°íšŒ)
     * - ì˜¤ë˜ëœ ë°ì´í„°(30ì¼ ì´ì „)ëŠ” DBì—ì„œ, ìµœì‹  ë°ì´í„°ëŠ” Prometheusì—ì„œ ì¡°íšŒí•˜ì—¬ ë³‘í•©í•©ë‹ˆë‹¤.
     */
    @Override
    public StatisticsResponseDTO getTimeSeriesStatistics(StatisticsQueryRequestDTO request) {
        log.info("Fetching time series statistics: metric={}, start={}, end={}, period={}, aggregation={}",
                request.getMetricType(), request.getStartTime(), request.getEndTime(),
                request.getTimePeriod(), request.getAggregationType());

        if (!request.isValidDateFormat()) {
            throw new IllegalArgumentException("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. í˜•ì‹: yyyy-MM-dd HH:mm:ss");
        }

        LocalDateTime now = LocalDateTime.now();
        LocalDateTime requestStart = request.getStartTimeAsLocalDateTime();
        LocalDateTime requestEnd = request.getEndTimeAsLocalDateTime();
        LocalDateTime prometheusThreshold = now.minusDays(prometheusDays);

        List<StatisticsResponseDTO.DataPoint> allData = new ArrayList<>();

        // 1. PostgreSQL ì¡°íšŒ (Retention ê¸°ê°„ ì´ì „ ë°ì´í„°)
        if (requestStart.isBefore(prometheusThreshold)) {
            LocalDateTime dbEnd = requestEnd.isBefore(prometheusThreshold) ? requestEnd : prometheusThreshold;

            List<StatisticsVO> dbData = statisticsMapper.getStatisticsByPeriod(
                    request.getMetricType(), request.getTimePeriod(), request.getAggregationType(), requestStart, dbEnd
            );

            List<StatisticsResponseDTO.DataPoint> dbPoints = dbData.stream()
                    .map(vo -> new StatisticsResponseDTO.DataPoint(
                            vo.getStartTime().atZone(ZoneId.systemDefault()).toEpochSecond(),
                            vo.getMetricValue().doubleValue(),
                            vo.getMinValue() != null ? vo.getMinValue().doubleValue() : null,
                            vo.getMaxValue() != null ? vo.getMaxValue().doubleValue() : null,
                            vo.getSampleCount()
                    ))
                    .collect(Collectors.toList());
            allData.addAll(dbPoints);
        }

        // 2. Prometheus ì¡°íšŒ (Retention ê¸°ê°„ ì´ë‚´ ë°ì´í„°)
        if (requestEnd.isAfter(prometheusThreshold)) {
            LocalDateTime promStart = requestStart.isAfter(prometheusThreshold) ? requestStart : prometheusThreshold;
            long start = promStart.atZone(ZoneId.systemDefault()).toEpochSecond();
            long end = requestEnd.atZone(ZoneId.systemDefault()).toEpochSecond();

            // ê¸°ê°„ì— ë”°ë¥¸ ì ì ˆí•œ Step(ê°„ê²©) ê³„ì‚°
            String step = calculateStep(request.getTimePeriod(), promStart, requestEnd);

            log.info("Fetching Prometheus Data. Step: {}", step);

            List<StatisticsResponseDTO.DataPoint> promPoints = fetchRichPrometheusData(
                    request.getMetricType(),
                    request.getAggregationType(),
                    start,
                    end,
                    step,
                    request.getApplication()
            );

            allData.addAll(promPoints);
        }

        // 3. ë°ì´í„° ë³‘í•© ë° ì‹œê°„ìˆœ ì •ë ¬
        allData.sort(Comparator.comparing(a -> LocalDateTime.parse(a.getTimestamp(), DATE_FORMATTER)));

        // 4. ì‘ë‹µ ìƒì„±
        StatisticsResponseDTO response = new StatisticsResponseDTO();
        response.setMetricType(request.getMetricType());
        response.setTimePeriod(request.getTimePeriod());
        response.setAggregationType(request.getAggregationType());
        response.setStartTime(requestStart.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setEndTime(requestEnd.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setDataSource(determineDataSource(requestStart, requestEnd, prometheusThreshold));
        response.setData(allData);

        return response;
    }

    /**
     * Prometheus ë°ì´í„° ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬)
     * - Main(ì„ íƒí•œ ì§‘ê³„), Min, Max ì¿¼ë¦¬ë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ Rich Dataë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
     */
    private List<StatisticsResponseDTO.DataPoint> fetchRichPrometheusData(
            String metricType, String mainAggregationType, long start, long end, String step, String application) {

        // 1. ì¿¼ë¦¬ ìƒì„± (ë©”ì¸ ì°¨íŠ¸ìš©, ìµœì†Œê°’ ë°´ë“œìš©, ìµœëŒ€ê°’ ë°´ë“œìš©)
        String mainQuery = buildPrometheusQuery(metricType, mainAggregationType, step, application);
        String minQuery = buildPrometheusQuery(metricType, "MIN", step, application);
        String maxQuery = buildPrometheusQuery(metricType, "MAX", step, application);

        // 2. ë¹„ë™ê¸° ë³‘ë ¬ ì‹¤í–‰ (Network I/O ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™”)
        CompletableFuture<List<Map<String, Object>>> mainFuture = CompletableFuture.supplyAsync(() ->
                prometheusService.queryRange(mainQuery, start, end, step));
        CompletableFuture<List<Map<String, Object>>> minFuture = CompletableFuture.supplyAsync(() ->
                prometheusService.queryRange(minQuery, start, end, step));
        CompletableFuture<List<Map<String, Object>>> maxFuture = CompletableFuture.supplyAsync(() ->
                prometheusService.queryRange(maxQuery, start, end, step));

        try {
            List<Map<String, Object>> mainData = mainFuture.get();
            List<Map<String, Object>> minData = minFuture.get();
            List<Map<String, Object>> maxData = maxFuture.get();

            if (mainData == null || mainData.isEmpty()) return Collections.emptyList();

            // 3. ë©”ì¸ ë°ì´í„° ë³€í™˜
            List<StatisticsResponseDTO.DataPoint> basePoints = prometheusStatisticsConverter.convertData(
                    mainData, step, mainAggregationType
            );

            // 4. Min, Max ë°ì´í„°ë¥¼ Mapìœ¼ë¡œ ë³€í™˜í•˜ì—¬ O(1)ë¡œ ì¡°íšŒ ê°€ëŠ¥í•˜ê²Œ ì²˜ë¦¬
            Map<String, Double> minMap = extractValueMapAsString(minData);
            Map<String, Double> maxMap = extractValueMapAsString(maxData);

            // 5. DataPointì— Min/Max ê°’ ë³‘í•©
            for (StatisticsResponseDTO.DataPoint point : basePoints) {
                String key = point.getTimestamp();
                double currentVal = point.getValue();

                point.setMinValue(minMap.getOrDefault(key, currentVal));
                point.setMaxValue(maxMap.getOrDefault(key, currentVal));

                if (point.getSampleCount() == null) point.setSampleCount(1);
            }

            return basePoints;

        } catch (InterruptedException | ExecutionException e) {
            log.error("Failed to fetch rich prometheus data", e);
            Thread.currentThread().interrupt(); // ì¸í„°ëŸ½íŠ¸ ìƒíƒœ ë³µêµ¬
            return Collections.emptyList();
        }
    }

    /**
     * Prometheus ì‘ë‹µ ë°ì´í„°ë¥¼ { "yyyy-MM-dd HH:mm:ss": Value } í˜•íƒœì˜ ë§µìœ¼ë¡œ ë³€í™˜
     */
    private Map<String, Double> extractValueMapAsString(List<Map<String, Object>> dataList) {
        Map<String, Double> map = new HashMap<>();
        if (dataList == null) return map;

        for (Map<String, Object> series : dataList) {
            List<List<Object>> values = (List<List<Object>>) series.get("values");
            if (values != null) {
                for (List<Object> valuePair : values) {
                    long timestampSeconds = ((Number) valuePair.get(0)).longValue();

                    String key = LocalDateTime.ofInstant(
                            java.time.Instant.ofEpochSecond(timestampSeconds),
                            ZoneId.systemDefault()
                    ).format(DATE_FORMATTER);

                    double value = Double.parseDouble(valuePair.get(1).toString());
                    map.put(key, value);
                }
            }
        }
        return map;
    }

    // --- Elasticsearch Log Statistics Methods ---

    @Override
    public LogStatisticsResponseDTO getLogStatistics(LogStatisticsQueryRequestDTO request) {
        if (!request.isValidDateFormat()) throw new IllegalArgumentException("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.");
        LocalDateTime startTime = request.getStartTimeAsLocalDateTime();
        LocalDateTime endTime = request.getEndTimeAsLocalDateTime();

        Map<String, Long> logCounts = elasticsearchService.countByLogLevel("application-logs-*");
        List<Map<String, Object>> distribution = elasticsearchService.getLogDistributionByTime(
                "application-logs-*", startTime, endTime, request.getTimePeriod(), request.getLogLevel()
        );

        LogStatisticsResponseDTO response = new LogStatisticsResponseDTO();
        response.setStartTime(startTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setEndTime(endTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setTimePeriod(request.getTimePeriod());
        response.setLogCounts(logCounts);
        response.setDistributions(logsConverter.toStatisticsDistribution(distribution));
        return response;
    }

    @Override
    public AccessLogStatisticsResponseDTO getAccessLogStatistics(AccessLogStatisticsQueryRequestDTO request) {
        log.info("Fetching access log statistics: start={}, end={}, period={}",
                request.getStartTime(), request.getEndTime(), request.getTimePeriod());
        if (!request.isValidDateFormat()) throw new IllegalArgumentException("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.");

        LocalDateTime startTime = request.getStartTimeAsLocalDateTime();
        LocalDateTime endTime = request.getEndTimeAsLocalDateTime();

        Map<String, Long> methodCounts = elasticsearchService.countByHttpMethod("access-logs-*", startTime, endTime);
        Map<String, Long> statusCodeCounts = elasticsearchService.countByStatusCode("access-logs-*", startTime, endTime);
        Double avgResponseTIme = elasticsearchService.getAverageResponseTime("access-logs-*", startTime, endTime);
        List<Map<String, Object>> distribution = elasticsearchService.getAccessLogDistributionByTime(
                "access-logs-*", startTime, endTime, request.getTimePeriod()
        );

        AccessLogStatisticsResponseDTO response = new AccessLogStatisticsResponseDTO();
        response.setStartTime(startTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setEndTime(endTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setTimePeriod(request.getTimePeriod());
        response.setMethodCounts(methodCounts);
        response.setStatusCodeCounts(statusCodeCounts);
        response.setAvgResponseTime(avgResponseTIme);
        response.setDistributions(accessLogsConverter.toStatisticsDistribution(distribution));
        return response;
    }

    @Override
    public ErrorLogStatisticsResponseDTO getErrorLogStatistics(ErrorLogStatisticsQueryRequestDTO request) {
        log.info("Fetching error log statistics: start={}, end={}, period={}",
                request.getStartTime(), request.getEndTime(), request.getTimePeriod());
        if (!request.isVaildDateFormat()) throw new IllegalArgumentException("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.");

        LocalDateTime startTime = request.getStartTimeAsLocalDateTime();
        LocalDateTime endTime = request.getEndTimeAsLocalDateTime();

        Map<String, Long> errorTypeCounts = elasticsearchService.countByErrorType("error-logs-*", startTime, endTime);
        Map<String, Long> severityCounts = elasticsearchService.countBySeverity("error-logs-*", startTime, endTime);
        List<Map<String, Object>> distribution = elasticsearchService.getErrorLogDistributionByTime(
                "error-logs-*", startTime, endTime, request.getTimePeriod()
        );

        ErrorLogStatisticsResponseDTO response = new ErrorLogStatisticsResponseDTO();
        response.setStartTime(startTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setEndTime(endTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setTimePeriod(request.getTimePeriod());
        response.setErrorTypeCounts(errorTypeCounts);
        response.setSeverityCounts(severityCounts);
        response.setDistributions(new ErrorLogsConverter().toStatisticsDistribution(distribution));
        return response;
    }

    @Override
    public PerformanceMetricsStatisticsResponseDTO getPerformanceMetricsStatistics(PerformanceMetricsStatisticsQueryRequestDTO request) {
        log.info("Fetching performance metrics statistics: start={}, end={}, period={}",
                request.getStartTime(), request.getEndTime(), request.getTimePeriod());
        if (!request.isValidDateFormat()) throw new IllegalArgumentException("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.");

        LocalDateTime startTime = request.getStartTimeAsLocalDateTime();
        LocalDateTime endTime = request.getEndTimeAsLocalDateTime();

        Map<String, Double> systemMetrics = elasticsearchService.getSystemMetricsAggregation("performance-metrics-*", startTime, endTime);
        Map<String, Double> jvmMetrics = elasticsearchService.getJvmMetricsAggregation("performance-metrics-*", startTime, endTime);
        List<Map<String, Object>> distribution = elasticsearchService.getPerformanceMetricsDistributionByTime(
                "performance-metrics-*", startTime, endTime, request.getTimePeriod()
        );

        PerformanceMetricsStatisticsResponseDTO response = new PerformanceMetricsStatisticsResponseDTO();
        response.setStartTime(startTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setEndTime(endTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setTimePeriod(request.getTimePeriod());

        response.setSystemMetrics(new PerformanceMetricsStatisticsResponseDTO.SystemMetrics(
                systemMetrics.getOrDefault("avg_cpu", 0.0), systemMetrics.getOrDefault("avg_memory", 0.0),
                systemMetrics.getOrDefault("avg_disk", 0.0), systemMetrics.getOrDefault("max_cpu", 0.0),
                systemMetrics.getOrDefault("max_memory", 0.0)
        ));

        response.setJvmMetrics(new PerformanceMetricsStatisticsResponseDTO.JvmMetrics(
                jvmMetrics.getOrDefault("avg_heap", 0.0), jvmMetrics.getOrDefault("max_heap", 0.0),
                jvmMetrics.getOrDefault("total_gc_count", 0.0).longValue(),
                jvmMetrics.getOrDefault("total_gc_time", 0.0).longValue(),
                jvmMetrics.getOrDefault("avg_thread_count", 0.0)
        ));

        response.setDistributions(new PerformanceMetricsConverter().toStatisticsDistribution(distribution));
        return response;
    }

    @Override
    public DatabaseLogStatisticsResponseDTO getDatabaseLogStatistics(DatabaseLogStatisticsQueryRequestDTO request) {
        log.info("Fetching database log statistics: start={}, end={}, period={}",
                request.getStartTime(), request.getEndTime(), request.getTimePeriod());
        if (!request.isValidDateFormat()) throw new IllegalArgumentException("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.");

        LocalDateTime startTime = request.getStartTimeAsLocalDateTime();
        LocalDateTime endTime = request.getEndTimeAsLocalDateTime();

        Map<String, Long> operationCounts = elasticsearchService.countByOperation("database-logs-*", startTime, endTime);
        Map<String, Long> tableCounts = elasticsearchService.countByTable("database-logs-*", startTime, endTime);
        Map<String, Object> queryPerformanceStats = elasticsearchService.getQueryPerformanceStats("database-logs-*", startTime, endTime);
        List<Map<String, Object>> distribution = elasticsearchService.getDatabaseLogDistributionByTime(
                "database-logs-*", startTime, endTime, request.getTimePeriod()
        );

        DatabaseLogStatisticsResponseDTO response = new DatabaseLogStatisticsResponseDTO();
        response.setStartTime(startTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setEndTime(endTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setTimePeriod(request.getTimePeriod());
        response.setOperationCounts(operationCounts);
        response.setTableCounts(tableCounts);

        response.setQueryPerformance(new DatabaseLogStatisticsResponseDTO.QueryPerformance(
                (Double) queryPerformanceStats.get("avgDuration"), (Double) queryPerformanceStats.get("maxDuration"),
                (Long) queryPerformanceStats.get("slowQueryCount"), (Long) queryPerformanceStats.get("totalQueryCount")
        ));

        response.setDistributions(new DatabaseLogsConverter().toStatisticsDistribution(distribution));
        return response;
    }

    @Override
    public AuditLogStatisticsResponseDTO getAuditLogStatistics(AuditLogStatisticsQueryRequestDTO request) {
        log.info("Fetching audit log statistics: start={}, end={}, period={}",
                request.getStartTime(), request.getEndTime(), request.getTimePeriod());
        if (!request.isValidDateFormat()) throw new IllegalArgumentException("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.");

        LocalDateTime startTime = request.getStartTimeAsLocalDateTime();
        LocalDateTime endTime = request.getEndTimeAsLocalDateTime();

        Map<String, Long> eventActionCounts = elasticsearchService.countByEventAction("audit-logs-*", startTime, endTime);
        Map<String, Long> categoryCounts = elasticsearchService.countByCategory("audit-logs-*", startTime, endTime);
        Map<String, Long> eventResultCounts = elasticsearchService.countByEventResult("audit-logs-*", startTime, endTime);
        List<Map<String, Object>> distribution = elasticsearchService.getAuditLogDistributionByTime(
                "audit-logs-*", startTime, endTime, request.getTimePeriod()
        );

        AuditLogStatisticsResponseDTO response = new AuditLogStatisticsResponseDTO();
        response.setStartTime(startTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setEndTime(endTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setTimePeriod(request.getTimePeriod());
        response.setEventActionCounts(eventActionCounts);
        response.setCategoryCounts(categoryCounts);

        Long successCount = eventResultCounts.getOrDefault("success", 0L);
        Long failureCount = eventResultCounts.getOrDefault("failure", 0L);
        Long totalCount = successCount + failureCount;
        Double successRate = totalCount > 0 ? (successCount * 100.0 / totalCount) : 0.0;
        response.setResultStats(new AuditLogStatisticsResponseDTO.ResultStats(successCount, failureCount, successRate));

        response.setDistributions(new AuditLogsConverter().toStatisticsDistribution(distribution));
        return response;
    }

    @Override
    public SecurityLogStatisticsResponseDTO getSecurityLogStatistics(SecurityLogStatisticsQueryRequestDTO request) {
        log.info("Fetching security log statistics: start={}, end={}, period={}",
                request.getStartTime(), request.getEndTime(), request.getTimePeriod());
        if (!request.isVaildDateFormat()) throw new IllegalArgumentException("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.");

        LocalDateTime startTime = request.getStartTimeAsLocalDateTime();
        LocalDateTime endTime = request.getEndTimeAsLocalDateTime();

        Map<String, Long> threatLevelCounts = elasticsearchService.countByThreatLevel("security-logs-*", startTime, endTime);
        Map<String, Long> attackTypeCounts = elasticsearchService.countByAttackType("security-logs-*", startTime, endTime);
        Map<String, Long> blockStatistics = elasticsearchService.getBlockStatistics("security-logs-*", startTime, endTime);
        List<Map<String, Object>> distribution = elasticsearchService.getSecurityLogDistributionByTime(
                "security-logs-*", startTime, endTime, request.getTimePeriod()
        );

        SecurityLogStatisticsResponseDTO response = new SecurityLogStatisticsResponseDTO();
        response.setStartTime(startTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setEndTime(endTime.atZone(ZoneId.systemDefault()).toEpochSecond());
        response.setTimePeriod(request.getTimePeriod());
        response.setThreatLevelCounts(threatLevelCounts);
        response.setAttackTypeCounts(attackTypeCounts);

        Long totalAttacks = blockStatistics.getOrDefault("totalAttacks", 0L);
        Long blockedAttacks = blockStatistics.getOrDefault("blockedAttacks", 0L);
        Long allowedAttacks = blockStatistics.getOrDefault("allowedAttacks", 0L);
        Double blockRate = totalAttacks > 0 ? (blockedAttacks * 100.0 / totalAttacks) : 0.0;
        response.setBlockStats(new SecurityLogStatisticsResponseDTO.BlockStats(totalAttacks, blockedAttacks, allowedAttacks, blockRate));

        response.setDistributions(new SecurityLogsConverter().convertToSecurityDistribution(distribution));
        return response;
    }

    // --- Prometheus Query Logic ---

    /**
     * Prometheus Query ìƒì„±ê¸°
     * - Multi-Instance í™˜ê²½ì—ì„œì˜ ì •í™•í•œ í†µê³„ë¥¼ ìœ„í•´ 'ê³µê°„ ì§‘ê³„(Spatial Aggregation)'ì™€ 'ì‹œê°„ ì§‘ê³„(Temporal Aggregation)'ë¥¼ êµ¬ë¶„í•˜ì—¬ ì ìš©í•©ë‹ˆë‹¤.
     * * @param metricType ë©”íŠ¸ë¦­ ì¢…ë¥˜ (CPU_USAGE, HEAP_USAGE ë“±)
     * @param aggregationType ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì§‘ê³„ ë°©ì‹ (AVG, MAX ë“±)
     * @param step Prometheus Query Resolution
     */
    private String buildPrometheusQuery(String metricType, String aggregationType, String step, String application) {
        // 1. ì‹œê°„ ì§‘ê³„ í•¨ìˆ˜ (ì˜ˆ: avg_over_time): ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë³€í™”ë¥¼ ê³„ì‚°
        String timeAggFunc = convertToPrometheusFunction(aggregationType);

        // 2. ê³µê°„ ì§‘ê³„ í•¨ìˆ˜ (ì˜ˆ: avg, max): ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤(Pod)ì˜ ê°’ì„ í•˜ë‚˜ë¡œ ë³‘í•©
        // ì‚¬ìš©ìê°€ MAXë¥¼ ì¡°íšŒí•˜ë©´ 'ê°€ì¥ ë°”ìœ ì„œë²„'ë¥¼ ì¶”ì í•´ì•¼ í•˜ë¯€ë¡œ ê³µê°„ ì§‘ê³„ë„ max(...)ë¥¼ ì‚¬ìš©
        String spaceAggFunc = convertToSpatialFunction(aggregationType);

        String resolution = "1m"; // ê¸°ë³¸ í•´ìƒë„

        // âœ… 1. Selector ìƒì„± ({application="eng-study"} í˜•íƒœ)
        // Prometheus ì„¤ì •ì— ë”°ë¼ label í‚¤ê°€ 'application', 'job', 'service' ì¤‘ ë¬´ì—‡ì¸ì§€ í™•ì¸ í•„ìš” (ë³´í†µ application ê¶Œì¥)
        String selector = (application != null && !application.isBlank())
                ? String.format("{application=\"%s\"}", application)
                : "";

        // Case 1: CPU Usage
        if ("CPU_USAGE".equalsIgnoreCase(metricType)) {
            // ì˜ˆ: process_cpu_usage{application="eng-study"}
            return String.format("%s((%s(process_cpu_usage%s))[%s:%s]) * 100",
                    timeAggFunc, spaceAggFunc, selector, step, resolution);
        }

        // Case 2: Heap Usage
        if ("HEAP_USAGE".equalsIgnoreCase(metricType)) {
            // Heapì€ area="heap" ì¡°ê±´ì´ í•„ìˆ˜ì´ë¯€ë¡œ, selectorì™€ í•©ì³ì•¼ í•¨
            // ì˜ˆ: jvm_memory_used_bytes{application="eng-study", area="heap"}
            String innerSelector = selector.isEmpty() ? "{area=\"heap\"}" : selector.replace("}", ", area=\"heap\"}");

            String heapExpr = String.format("(sum(jvm_memory_used_bytes%s) / sum(jvm_memory_max_bytes%s))", innerSelector, innerSelector);
            return String.format("%s((%s)[%s:%s]) * 100", timeAggFunc, heapExpr, step, resolution);
        }

        // Case 3: Counter Metrics (TPS, Error Rate)
        if (isCounterMetric(metricType)) {
            // âœ… selector ì „ë‹¬
            String baseRate = getRateExpression(metricType, resolution, selector);

            // SUMì¸ ê²½ìš° increase í•¨ìˆ˜ ì‚¬ìš©
            if ("SUM".equalsIgnoreCase(aggregationType)) {
                return getIncreaseExpression(metricType, step, selector);
            }

            return switch (aggregationType.toUpperCase()) {
                case "AVG" -> String.format("avg_over_time((%s)[%s:%s])", baseRate, step, resolution);
                case "MAX" -> String.format("max_over_time((%s)[%s:%s])", baseRate, step, resolution);
                case "MIN" -> String.format("min_over_time((%s)[%s:%s])", baseRate, step, resolution);
                default -> String.format("avg_over_time((%s)[%s:%s])", baseRate, step, resolution);
            };
        }

        // --- ğŸ˜ PostgreSQL ë©”íŠ¸ë¦­ ---

        // 1. í™œì„± ì—°ê²° ìˆ˜ (Connections)
        if ("DB_CONNECTIONS".equalsIgnoreCase(metricType)) {
            // pg_stat_activity_count
            return String.format("%s((sum(pg_stat_activity_count%s))[%s:%s])",
                    timeAggFunc, selector, step, resolution);
        }

        // 2. DB ì‚¬ì´ì¦ˆ (Bytes -> MB ë³€í™˜ ë“±ì€ í”„ë¡ íŠ¸ì—ì„œ í•˜ê±°ë‚˜ ì—¬ê¸°ì„œ /1024/1024)
        if ("DB_SIZE".equalsIgnoreCase(metricType)) {
            // pg_database_size_bytes
            return String.format("%s((sum(pg_database_size_bytes%s))[%s:%s])",
                    timeAggFunc, selector, step, resolution);
        }

        // 3. íŠ¸ëœì­ì…˜ ìˆ˜ (Commit + Rollback) - Counter íƒ€ì…ì´ë¼ rate ì ìš©
        if ("DB_TRANSACTIONS".equalsIgnoreCase(metricType)) {
            // xact_commit + xact_rollback
            String query = String.format("sum(rate(pg_stat_database_xact_commit%s[%s])) + sum(rate(pg_stat_database_xact_rollback%s[%s]))",
                    selector, resolution, selector, resolution);
            return String.format("avg_over_time((%s)[%s:%s])", query, step, resolution);
        }

        // --- ğŸ” Elasticsearch ë©”íŠ¸ë¦­ ---

        // 1. ES JVM Heap ì‚¬ìš©ë¥  (ESë„ Java ê¸°ë°˜)
        if ("ES_JVM_HEAP".equalsIgnoreCase(metricType)) {
            // elasticsearch_jvm_memory_used_bytes / elasticsearch_jvm_memory_max_bytes
            String esSelector = selector.isEmpty() ? "{area=\"heap\"}" : selector.replace("}", ", area=\"heap\"}");
            String heapExpr = String.format("(sum(elasticsearch_jvm_memory_used_bytes%s) / sum(elasticsearch_jvm_memory_max_bytes%s))", esSelector, esSelector);
            return String.format("%s((%s)[%s:%s]) * 100", timeAggFunc, heapExpr, step, resolution);
        }

        // 2. ë°ì´í„° í¬ê¸° (Index Size)
        // âœ… [ìˆ˜ì • í›„] indices_store_size_bytes -> ì‹¤ì œ ì¸ë±ìŠ¤ ë°ì´í„° ìš©ëŸ‰ (KB ~ MB ë‹¨ìœ„ ì˜ˆìƒ)
        // 'sum'ì„ í•´ì•¼ ëª¨ë“  ì¸ë±ìŠ¤(primary + replica)ì˜ í•©ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
        if ("ES_DATA_SIZE".equalsIgnoreCase(metricType)) {
            // indices_store_size_bytes -> ì‹¤ì œ ì¸ë±ìŠ¤ ë°ì´í„° ìš©ëŸ‰ (KB ~ MB ë‹¨ìœ„ ì˜ˆìƒ)
            // 'sum'ì„ í•´ì•¼ ëª¨ë“  ì¸ë±ìŠ¤(primary + replica)ì˜ í•©ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
            return String.format("%s((sum(elasticsearch_indices_store_size_bytes%s))[%s:%s])",
                    timeAggFunc, selector, step, resolution);
        }

        // 3. ES ë…¸ë“œ CPU
        if ("ES_CPU".equalsIgnoreCase(metricType)) {
            // elasticsearch_process_cpu_percent
            return String.format("%s((avg(elasticsearch_process_cpu_percent%s))[%s:%s])",
                    timeAggFunc, selector, step, resolution);
        }

        // Default
        String metricName = metricType.toLowerCase();
        return String.format("%s((%s(%s%s))[%s:%s])", timeAggFunc, spaceAggFunc, metricName, selector, step, resolution);
    }

    /**
     * í†µê³„ íƒ€ì…ì— ë”°ë¥¸ Prometheus ê³µê°„ ì§‘ê³„(Spatial Aggregation) í•¨ìˆ˜ ë§¤í•‘
     * - ì˜ˆ: MAX ì¡°íšŒ ì‹œ ì—¬ëŸ¬ ì„œë²„ ì¤‘ ê°€ì¥ ë†’ì€ ê°’ì„ ê°€ì§„ ì„œë²„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ê¸° ìœ„í•´ 'max' ë°˜í™˜
     */
    private String convertToSpatialFunction(String aggregationType) {
        return switch (aggregationType.toUpperCase()) {
            case "MAX" -> "max";
            case "MIN" -> "min";
            case "SUM" -> "sum";
            case "COUNT" -> "count";
            default -> "avg"; // ê¸°ë³¸ê°’ì€ ì „ì²´ í‰ê· 
        };
    }

    /**
     * í†µê³„ íƒ€ì…ì— ë”°ë¥¸ Prometheus ì‹œê°„ ì§‘ê³„(Temporal Aggregation) í•¨ìˆ˜ ë§¤í•‘
     */
    private String convertToPrometheusFunction(String aggregationType) {
        return switch (aggregationType.toUpperCase()) {
            case "AVG" -> "avg_over_time";
            case "MAX" -> "max_over_time";
            case "MIN" -> "min_over_time";
            case "SUM" -> "sum_over_time";
            case "COUNT" -> "count_over_time";
            default -> "avg_over_time";
        };
    }

    private boolean isCounterMetric(String metricType) {
        return "TPS".equalsIgnoreCase(metricType) || "ERROR_RATE".equalsIgnoreCase(metricType);
    }

    private String getRateExpression(String metricType, String window, String selector) {
        if ("TPS".equalsIgnoreCase(metricType)) {
            // http_server_requests_seconds_count{application="eng-study"}[1m]
            return String.format("sum(rate(http_server_requests_seconds_count%s[%s]))", selector, window);
        } else if ("ERROR_RATE".equalsIgnoreCase(metricType)) {
            // ì—ëŸ¬ìœ¨: (5xx ì—ëŸ¬ / ì „ì²´ ìš”ì²­) * 100
            // selector ë³‘í•© ë¡œì§: {application="x"} -> {application="x", status=~"5.."}
            String errorSelector = selector.isEmpty()
                    ? "{status=~\"5..\"}"
                    : selector.replace("}", ", status=~\"5..\"}");

            return String.format(
                    "(sum(rate(http_server_requests_seconds_count%s[%s])) / sum(rate(http_server_requests_seconds_count%s[%s]))) * 100",
                    errorSelector, window, selector, window
            );
        }
        return "";
    }

    private String getIncreaseExpression(String metricType, String window, String selector) {
        if ("TPS".equalsIgnoreCase(metricType)) {
            return String.format("sum(increase(http_server_requests_seconds_count%s[%s]))", selector, window);
        } else if ("ERROR_RATE".equalsIgnoreCase(metricType)) {
            String errorSelector = selector.isEmpty()
                    ? "{status=~\"5..\"}"
                    : selector.replace("}", ", status=~\"5..\"}");
            return String.format("sum(increase(http_server_requests_seconds_count%s[%s]))", errorSelector, window);
        }
        return "";
    }

    /**
     * ì¡°íšŒ ê¸°ê°„(Duration)ì— ë”°ë¥¸ ì ì ˆí•œ Prometheus Step(ê°„ê²©) ê³„ì‚°
     * - ì§§ì€ ê¸°ê°„ì€ ì´˜ì´˜í•˜ê²Œ(15m), ê¸´ ê¸°ê°„ì€ ë„ë„í•˜ê²Œ(1d) ì¡°íšŒí•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
     */
    private String calculateStep(String requestTimePeriod, LocalDateTime start, LocalDateTime end) {
        long durationMinutes = java.time.Duration.between(start, end).toMinutes();
        String unit = requestTimePeriod == null ? "AUTO" : requestTimePeriod.toUpperCase();

        switch (unit) {
            case "MINUTE":
                return durationMinutes <= 60 ? "15m" : "30m";
            case "HOUR":
                return "1h";
            case "DAY":
                return "1d";
            case "WEEK":
                return "1w";
            case "MONTH":
                return "30d";
            default:
                if (durationMinutes <= 60) return "15m";
                if (durationMinutes <= 360) return "30m";      // 6ì‹œê°„
                if (durationMinutes <= 1440) return "1h";      // 24ì‹œê°„
                if (durationMinutes <= 10080) return "6h";     // 7ì¼
                return "1d";
        }
    }

    private String determineDataSource(LocalDateTime start, LocalDateTime end, LocalDateTime threshold) {
        boolean hasPrometheus = end.isAfter(threshold);
        boolean hasPostgres = start.isBefore(threshold);

        if (hasPrometheus && hasPostgres) {
            return "MIXED";
        } else if (hasPrometheus) {
            return "PROMETHEUS";
        } else {
            return "POSTGRESQL";
        }
    }
}