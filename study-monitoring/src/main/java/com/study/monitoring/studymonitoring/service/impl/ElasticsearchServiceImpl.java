package com.study.monitoring.studymonitoring.service.impl;

import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.elasticsearch._types.FieldValue;
import co.elastic.clients.elasticsearch._types.SortOrder;
import co.elastic.clients.elasticsearch._types.aggregations.Aggregate;
import co.elastic.clients.elasticsearch._types.aggregations.Aggregation;
import co.elastic.clients.elasticsearch._types.aggregations.FieldDateMath;
import co.elastic.clients.elasticsearch._types.aggregations.StringTermsBucket;
import co.elastic.clients.elasticsearch._types.query_dsl.BoolQuery;
import co.elastic.clients.elasticsearch._types.query_dsl.Query;
import co.elastic.clients.elasticsearch.core.SearchResponse;
import co.elastic.clients.elasticsearch.core.search.Hit;
import com.study.monitoring.studymonitoring.model.dto.response.PageResponseDTO;
import com.study.monitoring.studymonitoring.service.ElasticsearchService;
import com.study.monitoring.studymonitoring.util.ElasticsearchQueryUtil;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.stream.Collectors;

/**
 * Elasticsearch ì„œë¹„ìŠ¤ êµ¬í˜„
 * ì£¼ìš” ê°œì„ ì‚¬í•­:
 * 1. ElasticsearchQueryUtil í™œìš©
 * 2. ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
 * 3. null ì•ˆì „ì„± ê°œì„ 
 * 4. ë¡œê¹… ì¶”ê°€
 */
@Slf4j
@Service
@RequiredArgsConstructor
public class ElasticsearchServiceImpl implements ElasticsearchService {

    private final ElasticsearchClient elasticsearchClient;
    private static final DateTimeFormatter FORMATTER = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");

    @Override
    public Map<String, Object> searchLogs(
            String indexPattern,
            String keyword,
            String logLevel,
            LocalDateTime startDate,
            LocalDateTime endDate,
            int from,
            int size)
    {
        try {
            log.debug("Searching logs: index={}, keyword={}, logLevel={}, startDate={}, endDate={}, from={}, size={}",
                    indexPattern, keyword, logLevel, startDate, endDate, from, size);

            // 1. Bool ì¿¼ë¦¬ ë¹Œë“œ
            BoolQuery.Builder boolQuery = new BoolQuery.Builder();

            // í‚¤ì›Œë“œ ê²€ìƒ‰ (Full-text Search)
            if (keyword != null && !keyword.isEmpty()) {
                boolQuery.must(ElasticsearchQueryUtil.buildMultiFieldSearchQuery(keyword));
            }

            // ë¡œê·¸ ë ˆë²¨ í•„í„°
            if (logLevel != null && !logLevel.isEmpty()) {
               boolQuery.must(ElasticsearchQueryUtil.buildLogLevelQuery(logLevel));
            }

            // ë‚ ì§œ ë²”ìœ„ í•„í„°
            if (startDate != null && endDate != null) {
                boolQuery.must(ElasticsearchQueryUtil.buildDateRangeQuery(startDate, endDate));
            } else if (startDate != null) {
                // ì‹œì‘ ë‚ ì§œë§Œ ìˆëŠ” ê²½ìš° (ì´í›„ ëª¨ë“  ë¡œê·¸)
                boolQuery.must(ElasticsearchQueryUtil.buildDateRangeQueryFrom(startDate));
            } else if (endDate != null) {
                // ì¢…ë£Œ ë‚ ì§œë§Œ ìˆëŠ” ê²½ìš° (ì´ì „ ëª¨ë“  ë¡œê·¸)
                boolQuery.must(ElasticsearchQueryUtil.buildDateRangeQueryTo(endDate));
            }

            // 2. Elasticsearch ê²€ìƒ‰ ì‹¤í–‰
            SearchResponse<Map> response = elasticsearchClient.search(
                    s -> s.index(indexPattern)
                            .from(from)
                            .size(size)
                            .query(boolQuery.build()._toQuery())
                            .sort(so -> so.field(f -> f.field("@timestamp").order(SortOrder.Desc))),
                    Map.class
            );

            // 3. ê²°ê³¼ ë³€í™˜
            List<Map<String, Object>> logs = response.hits().hits().stream()
                    .map(this::convertHitToMap)
                    .collect(Collectors.toList());

            // 4. ì‘ë‹µ êµ¬ì„±
            Map<String, Object> result = new HashMap<>();
            result.put("total", response.hits().total() != null ? response.hits().total().value() : 0);
            result.put("logs", logs);
            result.put("took", response.took());

            log.debug("Found {} logs", result.get("total"));
            return result;

        } catch (Exception e) {
            log.error("Failed to search logs: indexPattern={}", indexPattern, e);
            return createErrorResponse("ë¡œê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: " + e.getMessage());
        }
    }

    @Override
    public Map<String, Long> countByLogLevel(String indexPattern) {
        try {
            log.debug("Counting logs by level: index={}", indexPattern);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0)
                            .aggregations("by_log_level", Aggregation.of(a -> a
                                    .terms(t -> t
                                            .field("log_level.keyword").size(10)))), Void.class
            );
            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_log_level") != null) {
                response.aggregations().get("by_log_level").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()));
            }
            log.debug("Log level counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by log level: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public List<Map<String, Object>> getRecentErrors(int limit) {
        try {
            SearchResponse<Map> response = elasticsearchClient.search(s -> s
                            .index("application-logs-*,error-logs-*") // ì „ì²´ ë¡œê·¸ ëŒ€ìƒ
                            .size(limit)
                            .query(q -> q.terms(t -> t
                                    .field("log_level.keyword")
                                    .terms(v -> v.value(List.of(
                                            FieldValue.of("ERROR"),
                                            FieldValue.of("CRITICAL"),
                                            FieldValue.of("FATAL")
                                    )))
                            ))
                            .sort(so -> so.field(f -> f.field("@timestamp").order(SortOrder.Desc))),
                    Map.class
            );

            return response.hits().hits().stream()
                    .map(hit -> {
                        Map<String, Object> map = this.convertHitToMap(hit);
                        String realLevel = resolveLogLevel(map); // âœ… ì—¬ê¸°ë„ ì ìš©!

                        map.put("level", realLevel);
                        map.put("logLevel", realLevel);
                        map.put("log_level", realLevel);

                        return map;
                    })
                    .collect(Collectors.toList());

        } catch (Exception e) {
            log.error("Error fetching recent errors", e);
            return Collections.emptyList();
        }
    }

    // ì‹œê°„ëŒ€ ë³„ ë¶„í¬ - Application Logs
    @Override
    public List<Map<String, Object>> getLogDistributionByTime(
            String indexPattern,
            LocalDateTime startTime,
            LocalDateTime endTime,
            String timePeriod,
            String logLevel)
    {
        try {
            log.info("Querying log distribution: {} ~ {}, period={}, logLevel={}",
                    startTime, endTime, timePeriod, logLevel);

            String interval = calculateInterval(timePeriod);
            BoolQuery.Builder boolQuery = new BoolQuery.Builder();
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(startTime, endTime);
            boolQuery.must(timeRangeQuery);

            if (logLevel != null && !logLevel.isEmpty() && !"undefined".equals(logLevel)) {
                boolQuery.must(ElasticsearchQueryUtil.buildLogLevelQuery(logLevel));
            }

            // âœ… epoch millisecondsë¡œ ë³€í™˜
            long startEpochMs = startTime.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = endTime.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern)
                            .size(0)
                            .query(boolQuery.build()._toQuery())
                            .aggregations("logs_over_time",
                                    Aggregation.of(a -> a
                                            .dateHistogram(dh -> dh
                                                    .field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            // âœ… ìˆ˜ì •: epoch milliseconds ì‚¬ìš©
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                            )
                                    )
                            ),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("logs_over_time") != null) {
                response.aggregations().get("logs_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("count", bucket.docCount());
                            distribution.add(entry);
                        });
            }
            log.info("Distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get log distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public Map<String, Long> countByHttpMethod(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by HTTP method: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(s ->
                            s.index(indexPattern).size(0)
                                    .query(timeRangeQuery)
                                    .aggregations("by_method", Aggregation.of(a ->
                                            // âœ… ìˆ˜ì •: http.method -> http.method.keyword (nested object ë‚´ keyword í•„ë“œ)
                                            a.terms(t -> t.field("http.method.keyword").size(10))
                                    )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_method") != null) {
                response.aggregations().get("by_method").sterms().buckets().array()
                        .forEach(bucket -> counts.put(bucket.key().stringValue(), bucket.docCount()));
            }
            log.debug("HTTP method counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by HTTP method: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByStatusCode(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by status code: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(s ->
                            s.index(indexPattern).size(0)   // í†µê³„ ê²°ê³¼(ì§‘ê³„)ë§Œ í•„ìš”í•  ë•Œ ì‚¬ìš©
                            .query(timeRangeQuery).aggregations("by_status", Aggregation.of(a ->
                                    a.terms(t ->
                                    t.field("http.status_code").size(20))   // ìƒìœ„ 20ê°œì˜ ìƒíƒœ ì½”ë“œë§Œ ê°€ì ¸ì˜¤ê² ë‹¤.
                            )),
                    Void.class
            );
            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_status") != null) {
                response.aggregations().get("by_status").lterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                String.valueOf(bucket.key()),   // ìƒíƒœ ì½”ë“œ ê°’(ì˜ˆ: 200)
                                bucket.docCount()));            // ìƒíƒœ ì½”ë“œê°€ ë‚˜íƒ€ë‚œ íšŸìˆ˜
            }
            log.debug("Status code counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by status code: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Double getAverageResponseTime(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting average response time: index={}, start={}, end={}", indexPattern, start, end);
            Query timeResponseQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end); // íŠ¹ì • ì‹œê°„ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
            SearchResponse<Void> response = elasticsearchClient.search(s ->
                            s.index(indexPattern).size(0).query(timeResponseQuery)
                                    .aggregations("avg_response_time", Aggregation.of(a ->
                                            a.avg(avg -> avg.field("http.response_time_ms")))),
                    Void.class
            );

            if (response.aggregations() != null && response.aggregations().get("avg_response_time") != null) {
                Double avgValue = response.aggregations().get("avg_response_time").avg().value();
                log.debug("Average response time: {} ms", avgValue);
                return avgValue != null ? avgValue : 0.0;
            }

            return 0.0;
        } catch (Exception e) {
            log.error("Failed to get average response time indexPattern={}", indexPattern, e);
            return 0.0;
        }
    }

    // ì‹œê°„ëŒ€ë³„ ë¶„í¬ - Access Logs
    @Override
    public List<Map<String, Object>> getAccessLogDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying access log distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // âœ… epoch millisecondsë¡œ ë³€í™˜
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("access_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("avg_response_time", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("http.response_time_ms")))
                                    ).aggregations("error_count", Aggregation.of(sub ->
                                            sub.filter(f -> f.range(
                                                    r -> r.field("http.status_code")
                                                            .gte(co.elastic.clients.json.JsonData.of(500)))
                                            )
                                    ))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("access_over_time") != null) {
                response.aggregations().get("access_over_time").dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("requestCount", bucket.docCount());

                            Double avgResponseTime = bucket.aggregations().get("avg_response_time").avg().value();
                            entry.put("avgResponseTime", avgResponseTime != null ? avgResponseTime : 0.0);

                            Long errorCount = bucket.aggregations().get("error_count").filter().docCount();
                            entry.put("errorCount", errorCount);
                            distribution.add(entry);
                        });
            }
            log.info("Access log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get access log distribution", e);
            return Collections.emptyList();
        }
    }

    // ============================================
    // ğŸ”„ error-logs í†µê³„ìš© ë©”ì„œë“œ
    // ============================================
    @Override
    public Map<String, Long> countByErrorType(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by error type: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_error_type", Aggregation.of(
                                    // âœ… ìˆ˜ì •: .keyword ì œê±° ë˜ëŠ” ë‘˜ ë‹¤ ì‹œë„
                                    a -> a.terms(t -> t
                                            .field("error.type.keyword")  // 1ìˆœìœ„: keyword í•„ë“œ
                                            .size(20)
                                            .missing("UNKNOWN")           // null ì²˜ë¦¬
                                    )
                            )), Void.class
            );

            Map<String, Long> counts = new HashMap<>();

            // âœ… ì‘ë‹µì´ ë¹„ì–´ìˆìœ¼ë©´ .keyword ì—†ì´ ì¬ì‹œë„
            if (response.aggregations() != null &&
                    response.aggregations().get("by_error_type") != null &&
                    !response.aggregations().get("by_error_type").sterms().buckets().array().isEmpty()) {

                response.aggregations().get("by_error_type").sterms().buckets().array()
                        .forEach(bucket -> {
                            String key = bucket.key().stringValue();
                            long count = bucket.docCount();
                            log.info("âœ… Error type: {}, count: {}", key, count);
                            counts.put(key, count);
                        });
            } else {
                // âœ… Fallback: .keyword ì—†ì´ ì¬ì‹œë„
                log.warn("âš ï¸ error.type.keyword not found, trying error.type");
                response = elasticsearchClient.search(
                        s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                                .aggregations("by_error_type", Aggregation.of(
                                        a -> a.terms(t -> t
                                                .field("error.type")  // keyword ì—†ì´
                                                .size(20)
                                                .missing("UNKNOWN")
                                        )
                                )), Void.class
                );

                if (response.aggregations() != null &&
                        response.aggregations().get("by_error_type") != null) {
                    response.aggregations().get("by_error_type").sterms().buckets().array()
                            .forEach(bucket -> counts.put(bucket.key().stringValue(), bucket.docCount()));
                }
            }

            log.info("ğŸ“Š Final error type counts: {}", counts);
            return counts;

        } catch (Exception e) {
            log.error("âŒ Failed to count by error type: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countBySeverity(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by severity: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // âœ… ìˆ˜ì •: severity ëŒ€ì‹  log_level ì‚¬ìš© (Logstashê°€ í‘œì¤€í™”í•œ í•„ë“œ)
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern)
                            .size(0).query(timeRangeQuery)
                            .aggregations("by_severity", Aggregation.of(
                                    a -> a.terms(t -> t
                                            .field("log_level.keyword")  // â† ë³€ê²½!
                                            .size(10)
                                            .missing("UNKNOWN")
                                    )
                            )), Void.class
            );

            Map<String, Long> counts = new HashMap<>();

            if (response.aggregations() != null &&
                    response.aggregations().get("by_severity") != null &&
                    !response.aggregations().get("by_severity").sterms().buckets().array().isEmpty()) {

                response.aggregations().get("by_severity").sterms().buckets().array()
                        .forEach(bucket -> {
                            String key = bucket.key().stringValue();
                            long count = bucket.docCount();
                            log.info("âœ… Severity: {}, count: {}", key, count);
                            counts.put(key, count);
                        });
            } else {
                // âœ… Fallback: .keyword ì—†ì´ ì¬ì‹œë„
                log.warn("âš ï¸ log_level.keyword not found, trying log_level");
                response = elasticsearchClient.search(
                        s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                                .aggregations("by_severity", Aggregation.of(
                                        a -> a.terms(t -> t.field("log_level").size(10))
                                )), Void.class
                );

                if (response.aggregations() != null &&
                        response.aggregations().get("by_severity") != null) {
                    response.aggregations().get("by_severity").sterms().buckets().array()
                            .forEach(bucket -> counts.put(bucket.key().stringValue(), bucket.docCount()));
                }
            }

            log.info("ğŸ“Š Final severity counts: {}", counts);
            return counts;

        } catch (Exception e) {
            log.error("âŒ Failed to count by severity: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    // ì‹œê°„ëŒ€ë³„ ë¶„í¬ - Error Logs
    @Override
    public List<Map<String, Object>> getErrorLogDistributionByTime(String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying error log distribution: {} ~ {}, period={}", start, end, timePeriod);
            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("errors_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                    ).aggregations("error_type_breakdown", Aggregation.of(
                                            // âœ… ìˆ˜ì •: error.type -> error.type.keyword
                                            sub -> sub.terms(t -> t.field("error.type.keyword").size(5))
                                    ))
                            )),
                    Void.class
            );
            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("errors_over_time") != null) {
                response.aggregations().get("errors_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("errorCount", bucket.docCount());

                            // ì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬
                            Map<String, Long> errorTypeBreakdown = new HashMap<>();
                            bucket.aggregations().get("error_type_breakdown")
                                    .sterms().buckets().array()
                                    .forEach(typeBucket -> errorTypeBreakdown.put(
                                            typeBucket.key().stringValue(),
                                            typeBucket.docCount()));
                            entry.put("errorTypeBreakdown", errorTypeBreakdown);
                            distribution.add(entry);
                        });
            }
            log.info("Error log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get error log distribution: indexPattern={}", indexPattern, e);
            return Collections.emptyList();
        }
    }

    // ============================================
    // ğŸ”„ performance-metrics í†µê³„ìš© ë©”ì„œë“œ
    // ============================================
    @Override
    public Map<String, Double> getSystemMetricsAggregation(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting system metrics aggregation: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("avg_cpu", Aggregation.of(
                                    a -> a.avg(avg -> avg.field("system.cpu_usage"))
                            ))
                            .aggregations("max_cpu", Aggregation.of(
                                    a -> a.max(max -> max.field("system.cpu_usage"))
                            ))
                            .aggregations("avg_memory", Aggregation.of(
                                    a -> a.avg(avg -> avg.field("system.memory_usage"))
                            ))
                            .aggregations("max_memory", Aggregation.of(
                                    a -> a.max(max -> max.field("system.memory_usage"))
                            ))
                            .aggregations("avg_disk", Aggregation.of(
                                    a -> a.avg(avg -> avg.field("system.disk_usage"))
                            )), Void.class
            );
            Map<String, Double> metrics = new HashMap<>();
            if (response.aggregations() != null) {
                metrics.put("avg_cpu", getAggregationValue(response, "avg_cpu"));
                metrics.put("max_cpu", getAggregationValue(response, "max_cpu"));
                metrics.put("avg_memory", getAggregationValue(response, "avg_memory"));
                metrics.put("max_memory", getAggregationValue(response, "max_memory"));
                metrics.put("avg_disk", getAggregationValue(response, "avg_disk"));
            }
            log.debug("System metrics: {}", metrics);
            return metrics;
        } catch (Exception e) {
            log.error("Failed to get system metrics aggregation: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Double> getJvmMetricsAggregation(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting JVM metrics aggregation: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
        s -> s.index(indexPattern).size(0).query(timeRangeQuery).aggregations("avg_heap", Aggregation.of(
                a -> a.avg(avg -> avg.field("jvm.heap_used")))).aggregations("max_heap", Aggregation.of(
                    a -> a.max(max -> max.field("jvm.heap_used")))).aggregations("total_gc_count", Aggregation.of(
                        a -> a.sum(sum -> sum.field("jvm.gc_count")))).aggregations("total_gc_time", Aggregation.of(
                            a -> a.sum(sum -> sum.field("jvm.gc_time")))).aggregations("avg_thread_count", Aggregation.of(
                                a -> a.avg(avg -> avg.field("jvm.thread_count")))),
                    Void.class
            );
            Map<String, Double> metrics = new HashMap<>();
            if (response.aggregations() != null) {
                metrics.put("avg_heap", getAggregationValue(response, "avg_heap"));
                metrics.put("max_heap", getAggregationValue(response, "max_heap"));
                metrics.put("total_gc_count", getAggregationValue(response, "total_gc_count"));
                metrics.put("total_gc_time", getAggregationValue(response, "total_gc_time"));
                metrics.put("avg_thread_count", getAggregationValue(response, "avg_thread_count"));
            }
            log.debug("JVM metrics: {}", metrics);
            return metrics;
        } catch (Exception e) {
            log.error("Failed to get JVM metrics aggregation: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    // ì‹œê°„ëŒ€ë³„ ë¶„í¬ - Performance Metrics
    @Override
    public List<Map<String, Object>> getPerformanceMetricsDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying performance metrics distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // âœ… epoch millisecondsë¡œ ë³€í™˜
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("metrics_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("avg_cpu_usage", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("system.cpu_usage")))
                                    ).aggregations("avg_memory_usage", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("system.memory_usage")))
                                    ).aggregations("avg_heap_usage", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("jvm.heap_used"))))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("metrics_over_time") != null) {
                response.aggregations().get("metrics_over_time").dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("cpuUsage", getBucketAggregationValue(bucket, "avg_cpu_usage"));
                            entry.put("memoryUsage", getBucketAggregationValue(bucket, "avg_memory_usage"));
                            entry.put("heapUsage", getBucketAggregationValue(bucket, "avg_heap_usage"));
                            distribution.add(entry);
                        });
            }
            log.info("Performance metrics distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get performance metrics distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public Map<String, Long> countByOperation(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by operation: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_operation", Aggregation.of(
                                    // âœ… ìˆ˜ì •: operation -> operation.keyword
                                    a -> a.terms(t -> t.field("operation.keyword").size(10))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_operation") != null) {
                response.aggregations().get("by_operation").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Operation counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by operation: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByTable(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by table: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_table", Aggregation.of(
                                    // âœ… ìˆ˜ì •: table -> table.keyword
                                    a -> a.terms(t -> t.field("table.keyword").size(20))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_table") != null) {
                response.aggregations().get("by_table").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount())
                        );
            }
            log.debug("Table counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by table: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Object> getQueryPerformanceStats(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting query performance stats: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("avg_duration", Aggregation.of(
                                    a -> a.avg(avg -> avg.field("query.duration_ms")) // í•„ë“œëª… í™•ì¸ í•„ìš” (ì•„ë˜ ì„¤ëª… ì°¸ì¡°)
                            ))
                            .aggregations("max_duration", Aggregation.of(a -> a
                                    .max(max -> max.field("query.duration_ms"))
                            ))
                            .aggregations("slow_queries", Aggregation.of(a -> a
                                    .filter(f -> f
                                            .range(r -> r
                                                    .field("query.duration_ms")
                                                    .gte(co.elastic.clients.json.JsonData.of(1000))
                                            )))), Void.class
            );
            Map<String, Object> stats = new HashMap<>();
            if (response.aggregations() != null) {
                // 1. í‰ê· ê°’ ê°€ì ¸ì˜¤ê¸°
                Aggregate avgAggr = response.aggregations().get("avg_duration");
                double avg = (avgAggr != null && avgAggr.isAvg()) ? avgAggr.avg().value() : 0.0;

                // 2. ìµœëŒ€ê°’ ê°€ì ¸ì˜¤ê¸°
                Aggregate maxAggr = response.aggregations().get("max_duration");
                double max = (maxAggr != null && maxAggr.isMax()) ? maxAggr.max().value() : 0.0;

                // ë§Œì•½ ê°’ì´ ë¬´í•œëŒ€(Infinity)ë‚˜ NaNì´ë©´ 0ìœ¼ë¡œ ë³´ì • (ì•ˆì „ì¥ì¹˜)
                if (!Double.isFinite(avg)) avg = 0.0;
                if (!Double.isFinite(max)) max = 0.0;

                stats.put("avgDuration", avg);
                stats.put("maxDuration", max);

                // 3. ìŠ¬ë¡œìš° ì¿¼ë¦¬ ê°œìˆ˜ (ì—¬ê¸°ê°€ ì—ëŸ¬ ë°œìƒ ì§€ì )
                Aggregate slowAggr = response.aggregations().get("slow_queries");
                long slowCount = 0;
                if (slowAggr != null && slowAggr.isFilter()) {
                    slowCount = slowAggr.filter().docCount();
                }
                stats.put("slowQueryCount", slowCount);

                // 4. ì „ì²´ ì¿¼ë¦¬ ìˆ˜
                long totalCount = response.hits().total() != null ? response.hits().total().value() : 0;
                stats.put("totalQueryCount", totalCount);
            } else {
                // ì‘ë‹µì´ ë¹„ì–´ìˆì„ ê²½ìš° ê¸°ë³¸ê°’
                stats.put("avgDuration", 0.0);
                stats.put("maxDuration", 0.0);
                stats.put("slowQueryCount", 0L);
                stats.put("totalQueryCount", 0L);
            }

            log.debug("Query performance stats: {}", stats);
            return stats;
        } catch (Exception e) {
            log.error("Failed to get query performance stats: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    // ì‹œê°„ëŒ€ë³„ ë¶„í¬ - Database Logs
    @Override
    public List<Map<String, Object>> getDatabaseLogDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying database log distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // âœ… epoch millisecondsë¡œ ë³€í™˜
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("db_logs_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("avg_duration", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("query.duration_ms")))
                                    ).aggregations("slow_query_count", Aggregation.of(
                                            sub -> sub.filter(f -> f.range(
                                                    r -> r.field("query.duration_ms")
                                                            .gte(co.elastic.clients.json.JsonData.of(1000))))))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("db_logs_over_time") != null) {
                response.aggregations().get("db_logs_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("queryCount", bucket.docCount());
                            entry.put("avgDuration", getBucketAggregationValue(bucket, "avg_duration"));
                            entry.put("slowQueryCount", bucket.aggregations().get("slow_query_count").filter().docCount());
                            distribution.add(entry);
                        });
            }
            log.info("Database log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get database log distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public Map<String, Long> countByEventAction(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by event action: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_action", Aggregation.of(
                                    // âœ… ìˆ˜ì •: event.action.keyword (nested êµ¬ì¡°)
                                    a -> a.terms(t -> t.field("event.action.keyword").size(20))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_action") != null) {
                response.aggregations().get("by_action").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Event action counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to get event action", e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByCategory(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by category: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_category", a -> a.terms(
                                    // âœ… ìˆ˜ì •: event.category.keyword
                                    t -> t.field("event.category.keyword").size(10)
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_category") != null) {
                List<StringTermsBucket> buckets = response.aggregations().get("by_category").sterms().buckets().array();
                for (StringTermsBucket bucket : buckets) {
                    counts.put(bucket.key().stringValue(), bucket.docCount());
                }
            }
            log.debug("Category counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by category", e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByEventResult(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by event result: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_result", Aggregation.of(
                                    // âœ… ìˆ˜ì •: event.result.keyword
                                    a -> a.terms(t -> t.field("event.result.keyword").size(5))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_result") != null) {
                response.aggregations().get("by_result").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Event result counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by event result", e);
            return Collections.emptyMap();
        }
    }

    // ì‹œê°„ëŒ€ë³„ ë¶„í¬ - Audit Logs
    @Override
    public List<Map<String, Object>> getAuditLogDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying audit log distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // âœ… epoch millisecondsë¡œ ë³€í™˜
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("audit_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("success_count", Aggregation.of(
                                            sub -> sub.filter(f -> f.term(t -> t.field("event.result.keyword").value("success"))))
                                    ).aggregations("failure_count", Aggregation.of(
                                            sub -> sub.filter(f -> f.term(t -> t.field("event.result.keyword").value("failure")))))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("audit_over_time") != null) {
                response.aggregations().get("audit_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("totalEvents", bucket.docCount());

                            Long successCount = bucket.aggregations().get("success_count").filter().docCount();
                            Long failureCount = bucket.aggregations().get("failure_count").filter().docCount();
                            entry.put("successEvents", successCount);
                            entry.put("failureEvents", failureCount);
                            distribution.add(entry);
                        });
            }
            log.info("Audit log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get audit log distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public Map<String, Long> countByThreatLevel(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by threat level: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_threat_level", Aggregation.of(
                                    // âœ… ìˆ˜ì •: security.threat_level.keyword
                                    a -> a.terms(t -> t
                                            .field("security.threat_level.keyword")
                                            .size(10)
                                            .missing("unknown")
                                    )
                            )), Void.class
            );

            Map<String, Long> counts = new HashMap<>();

            if (response.aggregations() != null &&
                    response.aggregations().get("by_threat_level") != null &&
                    !response.aggregations().get("by_threat_level").sterms().buckets().array().isEmpty()) {

                response.aggregations().get("by_threat_level").sterms().buckets().array()
                        .forEach(bucket -> {
                            String key = bucket.key().stringValue();
                            long count = bucket.docCount();
                            log.info("âœ… Threat level: {}, count: {}", key, count);
                            counts.put(key, count);
                        });
            } else {
                // âœ… Fallback: .keyword ì—†ì´ ì¬ì‹œë„
                log.warn("âš ï¸ security.threat_level.keyword not found, trying without .keyword");
                response = elasticsearchClient.search(
                        s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                                .aggregations("by_threat_level", Aggregation.of(
                                        a -> a.terms(t -> t
                                                .field("security.threat_level")
                                                .size(10)
                                                .missing("unknown")
                                        )
                                )), Void.class
                );

                if (response.aggregations() != null &&
                        response.aggregations().get("by_threat_level") != null) {
                    response.aggregations().get("by_threat_level").sterms().buckets().array()
                            .forEach(bucket -> counts.put(bucket.key().stringValue(), bucket.docCount()));
                }
            }

            log.info("ğŸ“Š Final threat level counts: {}", counts);
            return counts;

        } catch (Exception e) {
            log.error("âŒ Failed to count by threat level: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByAttackType(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by attack type: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_attack_type", Aggregation.of(
                                    // âœ… ìˆ˜ì •: attack.type.keyword â†’ security.attack_type.keyword
                                    a -> a.terms(t -> t
                                            .field("security.attack_type.keyword")  // â† ë³€ê²½!
                                            .size(20)
                                            .missing("unknown")
                                    )
                            )), Void.class
            );

            Map<String, Long> counts = new HashMap<>();

            if (response.aggregations() != null &&
                    response.aggregations().get("by_attack_type") != null &&
                    !response.aggregations().get("by_attack_type").sterms().buckets().array().isEmpty()) {

                response.aggregations().get("by_attack_type").sterms().buckets().array()
                        .forEach(bucket -> {
                            String key = bucket.key().stringValue();
                            long count = bucket.docCount();
                            log.info("âœ… Attack type: {}, count: {}", key, count);
                            counts.put(key, count);
                        });
            } else {
                // âœ… Fallback: .keyword ì—†ì´ ì¬ì‹œë„
                log.warn("âš ï¸ security.attack_type.keyword not found, trying without .keyword");
                response = elasticsearchClient.search(
                        s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                                .aggregations("by_attack_type", Aggregation.of(
                                        a -> a.terms(t -> t
                                                .field("security.attack_type")
                                                .size(20)
                                                .missing("unknown")
                                        )
                                )), Void.class
                );

                if (response.aggregations() != null &&
                        response.aggregations().get("by_attack_type") != null) {
                    response.aggregations().get("by_attack_type").sterms().buckets().array()
                            .forEach(bucket -> counts.put(bucket.key().stringValue(), bucket.docCount()));
                }
            }

            log.info("ğŸ“Š Final attack type counts: {}", counts);
            return counts;

        } catch (Exception e) {
            log.error("âŒ Failed to count by attack type: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> getBlockStatistics(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting block statistics: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            // âœ… ë°©ë²• 1: boolean trueë¡œ ê²€ìƒ‰ (ë°ì´í„°ê°€ booleanì¼ ë•Œ)
                            .aggregations("blocked_attacks_bool", Aggregation.of(
                                    a -> a.filter(f -> f.term(t -> t.field("blocked").value(true)))
                            ))
                            // âœ… ë°©ë²• 2: String "true"ë¡œ ê²€ìƒ‰ (ë°ì´í„°ê°€ ë¬¸ìì—´ì¼ ë•Œ)
                            .aggregations("blocked_attacks_string", Aggregation.of(
                                    a -> a.filter(f -> f.term(t -> t.field("blocked.keyword").value("true")))
                            ))
                            // âœ… ë°©ë²• 3: String "false"ë¡œ ê²€ìƒ‰
                            .aggregations("allowed_attacks_string", Aggregation.of(
                                    a -> a.filter(f -> f.term(t -> t.field("blocked.keyword").value("false")))
                            ))
                            // âœ… ë°©ë²• 4: boolean falseë¡œ ê²€ìƒ‰
                            .aggregations("allowed_attacks_bool", Aggregation.of(
                                    a -> a.filter(f -> f.term(t -> t.field("blocked").value(false)))
                            )),
                    Void.class
            );

            Map<String, Long> stats = new HashMap<>();

            // ğŸ›¡ï¸ [ìˆ˜ì •] NPE ë°©ì§€: aggregations() ìì²´ê°€ nullì´ê±°ë‚˜, ê° í•­ëª©ì´ nullì¸ì§€ ì²´í¬
            if (response.aggregations() != null) {
                long totalAttacks = response.hits().total() != null ? response.hits().total().value() : 0;

                // Helper ë©”ì„œë“œë‚˜ ì‚¼í•­ ì—°ì‚°ìë¡œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
                long blockedBool = getDocCount(response.aggregations().get("blocked_attacks_bool"));
                long blockedString = getDocCount(response.aggregations().get("blocked_attacks_string"));
                long allowedBool = getDocCount(response.aggregations().get("allowed_attacks_bool"));
                long allowedString = getDocCount(response.aggregations().get("allowed_attacks_string"));

                long blockedAttacks = Math.max(blockedBool, blockedString);
                long allowedAttacks = Math.max(allowedBool, allowedString);

                log.info("ğŸ“Š Block stats - Total: {}, Blocked: {}, Allowed: {}", totalAttacks, blockedAttacks, allowedAttacks);

                stats.put("totalAttacks", totalAttacks);
                stats.put("blockedAttacks", blockedAttacks);
                stats.put("allowedAttacks", allowedAttacks);
            } else {
                stats.put("totalAttacks", 0L);
                stats.put("blockedAttacks", 0L);
                stats.put("allowedAttacks", 0L);
            }

            return stats;

        } catch (Exception e) {
            log.error("âŒ Failed to get block statistics: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    // ğŸ’¡ ì•ˆì „í•˜ê²Œ docCountë¥¼ êº¼ë‚´ëŠ” í—¬í¼ ë©”ì„œë“œ (í´ë˜ìŠ¤ ë‚´ë¶€ì— ì¶”ê°€í•˜ì„¸ìš”)
    private long getDocCount(Aggregate aggregate) {
        if (aggregate != null && aggregate.isFilter()) {
            return aggregate.filter().docCount();
        }
        return 0L;
    }

    // ì‹œê°„ëŒ€ë³„ ë¶„í¬ - Security Logs
    @Override
    public List<Map<String, Object>> getSecurityLogDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying security log distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("security_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("blocked_count_bool", Aggregation.of(
                                            sub -> sub.filter(f -> f.term(t -> t.field("blocked").value(true)))
                                    )).aggregations("blocked_count_string", Aggregation.of(
                                            sub -> sub.filter(f -> f.term(t -> t.field("blocked.keyword").value("true")))
                                    )).aggregations("threat_level_breakdown", Aggregation.of(
                                            sub -> sub.terms(t -> t.field("security.threat_level.keyword").size(5))
                                    ))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();

            // âœ… null ì²´í¬ ê°œì„ 
            if (response.aggregations() == null || response.aggregations().get("security_over_time") == null) {
                log.warn("No aggregations found in response");
                return distribution;
            }

            // âœ… íƒ€ì…ì„ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
            var securityOverTimeAgg = response.aggregations().get("security_over_time");
            if (securityOverTimeAgg == null || !securityOverTimeAgg.isDateHistogram()) {
                log.warn("security_over_time aggregation is not a date histogram");
                return distribution;
            }

            securityOverTimeAgg.dateHistogram().buckets().array()
                    .forEach(bucket -> {
                        Map<String, Object> entry = new HashMap<>();
                        entry.put("timestamp", bucket.keyAsString());
                        entry.put("attackCount", bucket.docCount());

                        // âœ… aggregations null ì²´í¬ ì¶”ê°€
                        if (bucket.aggregations() != null) {
                            // booleanê³¼ String ì¤‘ í° ê°’ ì‚¬ìš©
                            Long blockedBool = 0L;
                            Long blockedString = 0L;

                            var blockedBoolAgg = bucket.aggregations().get("blocked_count_bool");
                            if (blockedBoolAgg != null && blockedBoolAgg.isFilter()) {
                                blockedBool = blockedBoolAgg.filter().docCount();
                            }

                            var blockedStringAgg = bucket.aggregations().get("blocked_count_string");
                            if (blockedStringAgg != null && blockedStringAgg.isFilter()) {
                                blockedString = blockedStringAgg.filter().docCount();
                            }

                            Long blockedCount = Math.max(blockedBool, blockedString);
                            entry.put("blockedCount", blockedCount);

                            // threat_level_breakdown ì²˜ë¦¬
                            Map<String, Long> threatLevelBreakdown = new HashMap<>();
                            var threatLevelAgg = bucket.aggregations().get("threat_level_breakdown");

                            if (threatLevelAgg != null && threatLevelAgg.isSterms()) {
                                threatLevelAgg.sterms().buckets().array()
                                        .forEach(threatBucket ->
                                                threatLevelBreakdown.put(
                                                        threatBucket.key().stringValue(),
                                                        threatBucket.docCount()
                                                )
                                        );
                            }

                            entry.put("threatLevelBreakdown", threatLevelBreakdown);
                        } else {
                            // aggregationsê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                            entry.put("blockedCount", 0L);
                            entry.put("threatLevelBreakdown", new HashMap<String, Long>());
                        }

                        distribution.add(entry);
                    });

            log.info("Security log distribution result: {} time buckets", distribution.size());
            return distribution;

        } catch (Exception e) {
            log.error("Failed to get security log distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public PageResponseDTO<Map<String, Object>> searchErrorLogs(String type, int page, int size) {
        String indexName;

        if ("SYSTEM".equalsIgnoreCase(type)) {
            indexName = "error-logs-*";
        } else {
            indexName = "application-logs-*";
        }

        int currentPage = Math.max(1, page);
        int from = (currentPage - 1) * size;

        try {
            SearchResponse<Map> response = elasticsearchClient.search(s -> s
                            .index(indexName)
                            .from(from)
                            .size(size)
                            .query(q -> {
                                if ("SYSTEM".equalsIgnoreCase(type)) {
                                    return q.matchAll(m -> m);
                                } else {
                                    // ERROR, CRITICAL, FATAL ëª¨ë‘ ì¡°íšŒ
                                    return q.terms(t -> t
                                            .field("log_level.keyword")
                                            .terms(v -> v.value(List.of(
                                                    FieldValue.of("ERROR"),
                                                    FieldValue.of("CRITICAL"),
                                                    FieldValue.of("FATAL")
                                            )))
                                    );
                                }
                            })
                            .sort(so -> so.field(f -> f.field("@timestamp").order(SortOrder.Desc))),
                    Map.class
            );

            // âœ… [ìˆ˜ì •ë¨] ë¡œì§ ê°„ì†Œí™”
            // convertHitToMap ë‚´ë¶€ì—ì„œ ì´ë¯¸ determineLogLevelì„ í†µí•´
            // 500=ERROR, 503=CRITICAL ë¡œì§ì„ ìˆ˜í–‰í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë³€í™˜ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.
            List<Map<String, Object>> content = response.hits().hits().stream()
                    .map(this::convertHitToMap)
                    .collect(Collectors.toList());

            long totalElements = response.hits().total() != null ? response.hits().total().value() : 0;
            int totalPages = (int) Math.ceil((double) totalElements / size);

            return PageResponseDTO.<Map<String, Object>>builder()
                    .content(content)
                    .totalElements(totalElements)
                    .totalPages(totalPages)
                    .currentPage(currentPage)
                    .size(size)
                    .build();

        } catch (Exception e) {
            log.error("Error searching logs type={}", type, e);
            return PageResponseDTO.<Map<String, Object>>builder()
                    .content(Collections.emptyList())
                    .build();
        }
    }

    // [ì‹ ê·œ ì¶”ê°€] DashboardServiceì—ì„œ ê°€ì ¸ì˜¨ ë ˆë²¨ íŒë‹¨ ë¡œì§
    private String resolveLogLevel(Map<String, Object> logMap) {
        // 1. MDC í™•ì¸ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
        if (logMap.containsKey("mdc")) {
            Object mdcObj = logMap.get("mdc");
            if (mdcObj instanceof Map) {
                Map<String, Object> mdc = (Map<String, Object>) mdcObj;
                // MDC ë‚´ë¶€ì— severityë‚˜ log_level í‚¤ê°€ CRITICALì´ë©´ ê²©ìƒ
                if ("CRITICAL".equalsIgnoreCase((String) mdc.get("severity")) ||
                        "CRITICAL".equalsIgnoreCase((String) mdc.get("log_level"))) {
                    return "CRITICAL";
                }
            }
        }

        // 2. ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ë¶„ì„ (ì„ì‹œ ë°©í¸ì´ì ê°•ë ¥í•œ ê°•ì œ ìˆ˜ë‹¨)
        String message = (String) logMap.getOrDefault("message", "");
        if (message != null && (message.contains("Critical") || message.contains("ğŸš¨"))) {
            return "CRITICAL";
        }

        // 3. ìœ„ ì¡°ê±´ì— ì•ˆ ê±¸ë¦¬ë©´ ì›ë˜ DBì— ìˆë˜ ë ˆë²¨ ë°˜í™˜ (ì—†ìœ¼ë©´ ERROR)
        String originalLevel = (String) logMap.getOrDefault("logLevel",
                (String) logMap.getOrDefault("level", "ERROR"));

        return originalLevel;
    }

    // timePeriod â†’ Elasticsearch interval ë³€í™˜
    private String calculateInterval(String timePeriod) {
        return switch (timePeriod.toUpperCase()) {
            case "MINUTE" -> "1m";
            case "HOUR" -> "1h";
            case "DAY" -> "1d";
            case "WEEK" -> "7d";
            case "MONTH" -> "30d";
            default -> "1h";
        };
    }

    /**
     * Elasticsearch Hitì„ Mapìœ¼ë¡œ ë³€í™˜
     *
     * @param hit Hit ê°ì²´
     * @return Map
     */
    private Map<String, Object> convertHitToMap(Hit<Map> hit) {
        Map<String, Object> result = new HashMap<>();

        // ë¬¸ì„œ ID ë° ì¸ë±ìŠ¤ ì¶”ê°€
        result.put("_id", hit.id());
        result.put("_index", hit.index());

        // ì†ŒìŠ¤ ë°ì´í„°
        Map<String, Object> source = hit.source();
        if (source == null) {
            return result;
        }

        // ê³µí†µ í•„ë“œ: @timestamp, application
        result.put("@timestamp", source.get("@timestamp"));
        result.put("application", source.get("application"));

        // ì›ë³¸ì˜ MDCê°€ ìˆë‹¤ë©´ ë³µì‚¬ (Keyword ì²´í¬ ë“±ì„ ìœ„í•´)
        if (source.containsKey("mdc")) {
            result.put("mdc", source.get("mdc"));
        }

        // âœ… ì¸ë±ìŠ¤ íƒ€ì…ë³„ í•„ë“œ ë§¤í•‘
        String indexName = hit.index();

        // -------------------------------------------------------
        // ì¸ë±ìŠ¤ë³„ ë§¤í•‘ ë¡œì§ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•˜ë˜ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì •ë¦¬)
        // -------------------------------------------------------
        if (indexName.startsWith("application-logs")) {
            result.putAll(source);

        } else if (indexName.startsWith("access-logs")) {
            Map<String, Object> http = (Map<String, Object>) source.get("http");
            if (http != null) {
                result.put("http", http);
                // ë©”ì‹œì§€ í•„ë“œê°€ ì—†ìœ¼ë©´ ìƒì„±
                String msg = String.format("%s %s - Status: %s",
                        http.get("method"), http.get("url"), http.get("status_code"));
                result.put("message", msg);
            }
            if (source.containsKey("client")) result.put("client", source.get("client"));

        } else if (indexName.startsWith("error-logs")) {
            Map<String, Object> error = (Map<String, Object>) source.get("error");
            if (error != null) {
                // severityë¥¼ log_level í›„ë³´ë¡œ ì €ì¥
                result.put("log_level", error.get("severity"));
                result.put("logger_name", "ErrorLog");
                result.put("message", error.get("type") + ": " + error.get("message"));
                result.put("stack_trace", error.get("stack_trace"));
                result.put("error", error);
            }
            if (source.containsKey("source")) result.put("source", source.get("source"));

        } else if (indexName.startsWith("performance-metrics")) {
            // performance-metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ìˆ˜ì •ë¨)
            result.put("log_level", "INFO");

            // 1. ë©”ì„œë“œ ì‹¤í–‰ ì‹œê°„ ë¡œê·¸ì¸ì§€ í™•ì¸ (class, method, execution_time_ms í•„ë“œ ì¡´ì¬ ì—¬ë¶€)
            if (source.containsKey("method") && source.containsKey("execution_time_ms")) {
                // Logger Name: í´ë˜ìŠ¤ ì´ë¦„ ì‚¬ìš© (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
                Object className = source.get("class");
                result.put("logger_name", className != null ? className : "PerformanceLog");

                // Message: "Method Execution: checkLoginId - 4208ms" í˜•íƒœë¡œ ê°€ê³µ
                String message = String.format("Method Execution: %s - %sms",
                        source.get("method"),
                        source.get("execution_time_ms")
                );
                result.put("message", message);

                // ìƒì„¸ ë°ì´í„° ì›ë³¸ë„ í¬í•¨ (í”„ë¡ íŠ¸ì—”ë“œ ì •ë ¬/í•„í„°ë§ìš©)
                result.put("class", className);
                result.put("method", source.get("method"));
                result.put("execution_time_ms", source.get("execution_time_ms"));

            } else {
                // 2. ì‹œìŠ¤í…œ/JVM ë©”íŠ¸ë¦­ (ê¸°ì¡´ ë¡œì§ - í˜¹ì‹œ ì‹œìŠ¤í…œ ë¡œê·¸ê°€ ë“¤ì–´ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìœ ì§€)
                result.put("logger_name", "SystemMetrics");

                Map<String, Object> system = (Map<String, Object>) source.get("system");
                Map<String, Object> jvm = (Map<String, Object>) source.get("jvm");

                StringBuilder sb = new StringBuilder("System Metrics");
                if (system != null) {
                    sb.append(String.format(" - CPU: %s%%", system.get("cpu_usage")));
                    result.put("system", system);
                }
                if (jvm != null) {
                    result.put("jvm", jvm);
                }

                // ì›ë³¸ ë©”ì‹œì§€ê°€ "Performance Data" ì²˜ëŸ¼ ë‹¨ìˆœí•˜ë©´ ìƒì„¸ ì •ë³´ë¥¼, ì•„ë‹ˆë©´ ì›ë³¸ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©
                String originalMsg = (String) source.get("message");
                if (originalMsg != null && !originalMsg.equals("Performance Data")) {
                    result.put("message", originalMsg);
                } else {
                    result.put("message", sb.toString());
                }
            }

        } else if (indexName.startsWith("database-logs")) {
            // database-logs: ë°ì´í„°ë² ì´ìŠ¤ ë¡œê·¸

            // 1. ê¸°ì¡´ì²˜ëŸ¼ êµ¬ì¡°í™”ëœ ì¿¼ë¦¬ ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
            Map<String, Object> query = (Map<String, Object>) source.get("query");

            if (query != null) {
                // [Case A] êµ¬ì¡°í™”ëœ ë¡œê·¸ê°€ ë“¤ì–´ì˜¨ ê²½ìš° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                result.put("log_level", "INFO");
                result.put("logger_name", "DatabaseLog");

                String message = String.format("%s - %s (Duration: %sms)",
                        source.get("operation"),
                        source.get("table"),
                        query.get("duration_ms")
                );
                result.put("message", message);
                result.put("query", query);
                result.put("stack_trace", query.get("sql"));
                result.put("operation", source.get("operation"));
                result.put("table", source.get("table"));

            } else {
                // âœ… [Case B] ìš°ë¦¬ê°€ ë§Œë“  Interceptor ë¡œê·¸ (ì¼ë°˜ í…ìŠ¤íŠ¸ ë©”ì‹œì§€) ì²˜ë¦¬
                // êµ¬ì¡°í™”ëœ 'query' ê°ì²´ê°€ ì—†ë‹¤ë©´, ì›ë³¸ 'message' í•„ë“œë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

                // ë¡œê·¸ ë ˆë²¨ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ INFO)
                Object logLevel = source.get("log_level");
                result.put("log_level", logLevel != null ? logLevel : "INFO");

                // ë¡œê±° ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ DatabaseLog)
                Object loggerName = source.get("logger_name");
                result.put("logger_name", loggerName != null ? loggerName : "DatabaseLog");

                // â˜… í•µì‹¬: Interceptorê°€ ë§Œë“  "SQL: [...]" ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
                result.put("message", source.get("message"));

                // ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì •ë³´ê°€ ìˆë‹¤ë©´ ì¶”ê°€
                if (source.containsKey("stack_trace")) {
                    result.put("stack_trace", source.get("stack_trace"));
                }
            }

        } else if (indexName.startsWith("audit-logs")) {
            // audit-logs: ê°ì‚¬ ë¡œê·¸ (ìˆ˜ì •ë¨)

            // 1. ê¸°ë³¸ ì„¤ì • (ë°ì´í„°ì— ë ˆë²¨ì´ ì—†ìœ¼ë¯€ë¡œ INFOë¡œ ê³ ì •)
            result.put("log_level", "INFO");
            result.put("logger_name", "AuditLog");

            // 2. ë°ì´í„° ì¶”ì¶œ
            Map<String, Object> user = (Map<String, Object>) source.get("user");
            Map<String, Object> resource = (Map<String, Object>) source.get("resource");
            String originalMessage = (String) source.get("message");

            // 3. ë©”ì‹œì§€ ì¬êµ¬ì„± (ëˆ„ê°€, ë¬´ì—‡ì„ í–ˆëŠ”ì§€ ëª…í™•í•˜ê²Œ í‘œì‹œ)
            // ì˜ˆ: "User registration completed by test001 (Resource: í…ŒìŠ¤í„°)"
            StringBuilder messageBuilder = new StringBuilder();
            messageBuilder.append(originalMessage != null ? originalMessage : "Audit Event");

            if (user != null && user.get("login_id") != null) {
                messageBuilder.append(" by ").append(user.get("login_id"));
            }

            if (resource != null && resource.get("name") != null) {
                messageBuilder.append(" (Resource: ").append(resource.get("name")).append(")");
            }

            result.put("message", messageBuilder.toString());

            // 4. ìƒì„¸ ì •ë³´ ë‹´ê¸° (í”„ë¡ íŠ¸ì—”ë“œ ìƒì„¸ ëª¨ë‹¬ìš©)
            if (user != null) {
                result.put("user", user);
            }
            if (resource != null) {
                result.put("resource", resource);
            }

            // ê¸°ì¡´ 'event' ê°ì²´ê°€ ìˆë‹¤ë©´ ê°™ì´ ë„£ì–´ì¤Œ (í•˜ìœ„ í˜¸í™˜ì„±)
            if (source.containsKey("event")) {
                result.put("event", source.get("event"));
            }

        } else if (indexName.startsWith("security-logs")) {
            // =================================================
            // [SEC] ë³´ì•ˆ ë¡œê·¸ (ì§€ëŠ¥í˜• ë¶„ì„ ì ìš©)
            // =================================================

            // 1. êµ¬ì¡°í™”ëœ ë³´ì•ˆ ì´ë²¤íŠ¸ ê°ì²´(security, attack)ê°€ ìˆëŠ”ì§€ í™•ì¸ (WAF ë“± ì—°ë™ ì‹œ)
            Map<String, Object> security = (Map<String, Object>) source.get("security");
            Map<String, Object> attack = (Map<String, Object>) source.get("attack");

            if (security != null && attack != null) {
                // [Case A] êµ¬ì¡°í™”ëœ ìœ„í˜‘ ë¡œê·¸ ì²˜ë¦¬
                String threatLevel = (String) security.get("threat_level");

                // Threat Level -> Log Level ë§¤í•‘
                String logLevel;
                switch (threatLevel != null ? threatLevel.toLowerCase() : "low") {
                    case "critical": logLevel = "FATAL"; break;
                    case "high":     logLevel = "ERROR"; break;
                    case "medium":   logLevel = "WARN";  break;
                    default:         logLevel = "INFO";  break;
                }

                result.put("log_level", logLevel);
                result.put("logger_name", "SecurityEvent");
                result.put("message", String.format("[%s] Security Alert: %s (Blocked: %s)",
                        threatLevel, attack.get("type"), source.get("blocked")));

                result.put("security", security);
                result.put("attack", attack);

            } else {
                // [Case B] ì¼ë°˜ Spring Security í…ìŠ¤íŠ¸ ë¡œê·¸ ë¶„ì„
                String rawMessage = (String) source.get("message");
                Object originalLevelObj = source.get("level");
                String level = originalLevelObj != null ? originalLevelObj.toString() : "INFO";

                // ë¡œê±° ì´ë¦„ ì •ë¦¬ (íŒ¨í‚¤ì§€ëª… ë‹¨ì¶•)
                String loggerName = "SecurityLog";
                if (source.get("logger") != null) {
                    String fullLogger = source.get("logger").toString();
                    loggerName = fullLogger.contains(".")
                            ? fullLogger.substring(fullLogger.lastIndexOf(".") + 1)
                            : fullLogger;
                }

                // --- ğŸ” ë©”ì‹œì§€ ë¶„ì„ ë° ë ˆë²¨/ìœ í˜• ì¬ì •ì˜ ---
                String securityType = "General Event";

                if (rawMessage != null) {
                    // 1. ë¡œê·¸ì¸ ì‹¤íŒ¨
                    if (rawMessage.contains("Bad credentials") ||
                            rawMessage.contains("password does not match") ||
                            rawMessage.contains("User not found") ||
                            rawMessage.contains("Authentication failed")) {

                        securityType = "Login Failure";
                        level = "WARN"; // ê²©ìƒ

                        // 2. ê¶Œí•œ ì—†ìŒ (í•´í‚¹ ì‹œë„ ì˜ì‹¬)
                    } else if (rawMessage.contains("Access is denied") ||
                            rawMessage.contains("AccessDeniedException") ||
                            rawMessage.contains("AnonymouseAuthenticationToken")) {

                        securityType = "Access Denied";
                        level = "ERROR"; // ê²©ìƒ

                        // 3. CSRF ê³µê²©
                    } else if (rawMessage.contains("Invalid CSRF") ||
                            rawMessage.contains("Missing CSRF")) {

                        securityType = "CSRF Warning";
                        level = "ERROR"; // ê²©ìƒ

                        // 4. ì„¸ì…˜ ë§Œë£Œ
                    } else if (rawMessage.contains("Session") && rawMessage.contains("expired")) {

                        securityType = "Session Expired";
                        level = "WARN";
                    }
                }
                // ---------------------------------------

                result.put("log_level", level);
                result.put("logger_name", loggerName);
                result.put("message", rawMessage);
                result.put("security_type", securityType); // í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš© ìœ í˜•

                if (source.containsKey("tags")) {
                    result.put("tags", source.get("tags"));
                }
            }

        } else {
            result.putAll(source);
        }

        // ëª¨ë“  ë§¤í•‘ì´ ëë‚œ í›„, ìµœì¢…ì ìœ¼ë¡œ ë ˆë²¨ì„ 'íŒê²°'í•©ë‹ˆë‹¤.
        String realLevel = determineLogLevel(result);

        // íŒê²°ëœ ë ˆë²¨ì„ ëª¨ë“  ê´€ë ¨ í•„ë“œì— ë®ì–´ì”ë‹ˆë‹¤.
        result.put("log_level", realLevel);
        result.put("logLevel", realLevel);
        result.put("level", realLevel);

        // ë¡œê±° ì´ë¦„ì´ ë¹„ì–´ìˆë‹¤ë©´ ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì • (ì˜µì…˜)
        if (!result.containsKey("logger_name")) {
            if (indexName.startsWith("access")) result.put("logger_name", "AccessLog");
            else if (indexName.startsWith("security")) result.put("logger_name", "SecurityLog");
            else result.put("logger_name", "SystemLog");
        }

        return result;
    }

    /**
     * ë¡œê·¸ ë ˆë²¨ ìµœì¢… íŒì • (Smart Logic ì ìš©)
     * - 500(ì½”ë“œì—ëŸ¬) vs 503/504(ì„œë²„ì¥ì• ) êµ¬ë¶„
     * - ì¹˜ëª…ì ì¸ ì—ëŸ¬ í‚¤ì›Œë“œ ê°ì§€
     */
    private String determineLogLevel(Map<String, Object> doc) {
        String index = (String) doc.get("_index");
        String message = (String) doc.getOrDefault("message", "");

        // ë©”ì‹œì§€ê°€ nullì¼ ê²½ìš° ë°©ì–´ ë¡œì§
        if (message == null) message = "";

        // ==========================================
        // 1. [Access Logs] HTTP ìƒíƒœ ì½”ë“œ ì •ë°€ ë¶„ì„
        // ==========================================
        if (index != null && index.startsWith("access-logs")) {
            int status = extractHttpStatusCode(doc); // (ê¸°ì¡´ì— ì¡´ì¬í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ í™œìš©)

            // [CRITICAL] ì¸í”„ë¼ ì¥ì•  / ì„œë¹„ìŠ¤ ë¶ˆëŠ¥
            // 503: Service Unavailable (ì„œë²„ ê³¼ë¶€í•˜, ë°°í¬ ì¤‘)
            // 504: Gateway Timeout (DBë‚˜ ë°±ì—”ë“œ ì‘ë‹µ ì—†ìŒ)
            if (status == 503 || status == 504) {
                return "CRITICAL";
            }

            // [ERROR] ë°±ì—”ë“œ ì½”ë“œ ë²„ê·¸ / ë‚´ë¶€ ì—ëŸ¬
            // 500: Internal Server Error (NPE, ë¡œì§ ì˜¤ë¥˜)
            // 502: Bad Gateway
            if (status >= 500) {
                return "ERROR";
            }

            // [WARN] í´ë¼ì´ì–¸íŠ¸ ê³¼ì‹¤
            if (status >= 400) {
                return "WARN";
            }

            return "INFO";
        }

        // ==========================================
        // 2. [All Logs] ì¹˜ëª…ì ì¸ í‚¤ì›Œë“œ ê²€ì‚¬ (ê°•ì œ ìŠ¹ê²©)
        // ==========================================
        // ë¡œê·¸ ë ˆë²¨ì´ ë­ë“  ê°„ì—, ì´ ë‹¨ì–´ë“¤ì´ ë³´ì´ë©´ ë¬´ì¡°ê±´ CRITICALë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
        if (message.contains("OutOfMemory") ||
                message.contains("StackOverflow") ||
                message.contains("Deadlock") ||
                message.contains("Connection refused") ||
                message.contains("Fatal") ||
                message.contains("CRITICAL") ||  // ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²´í¬í•˜ë ¤ë©´ toUpperCase() ì‚¬ìš© ê¶Œì¥
                message.contains("ğŸš¨")) {
            return "CRITICAL";
        }

        // ==========================================
        // 3. [Security Logs] ìœ„í˜‘ ìˆ˜ì¤€ ê¸°ë°˜
        // ==========================================
        if (index != null && index.startsWith("security-logs")) {
            String threatLevel = extractThreatLevel(doc);
            if (threatLevel != null) {
                return switch (threatLevel.toLowerCase()) {
                    case "critical" -> "CRITICAL";
                    case "high" -> "ERROR";
                    case "medium" -> "WARN";
                    default -> "INFO";
                };
            }
        }

        // ==========================================
        // 4. [Audit Logs] ì‹¤íŒ¨ ì—¬ë¶€ ê¸°ë°˜
        // ==========================================
        if (index != null && index.startsWith("audit-logs")) {
            String eventResult = extractEventResult(doc);
            // ë¡œê·¸ì¸ ì‹¤íŒ¨ ë“±ì€ WARN ì²˜ë¦¬ê°€ ì ì ˆí•  ìˆ˜ ìˆìœ¼ë‚˜ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”ê±´ì— ë”°ë¼ ERROR ìœ ì§€
            return "failure".equalsIgnoreCase(eventResult) ? "ERROR" : "INFO";
        }

        // ==========================================
        // 5. ê¸°ë³¸ ë°˜í™˜ (ì›ë³¸ ë°ì´í„°ì˜ ë ˆë²¨ ì¡´ì¤‘)
        // ==========================================
        String[] levelFields = {"log_level", "logLevel", "level", "severity"};
        for (String field : levelFields) {
            Object value = doc.get(field);
            if (value != null && !value.toString().isEmpty() && !"null".equals(value.toString())) {
                String level = value.toString().toUpperCase();
                // í‘œì¤€ ë ˆë²¨ íŒ¨í„´ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
                if (level.matches("CRITICAL|FATAL|ERROR|WARN|INFO|DEBUG|TRACE")) {
                    return level;
                }
            }
        }

        return "INFO";
    }

    private int extractHttpStatusCode(Map<String, Object> doc) {
        if (doc.get("http") instanceof Map) {
            Map<String, Object> http = (Map<String, Object>) doc.get("http");
            Object statusCode = http.get("status_code");
            if (statusCode instanceof Number) {
                return ((Number) statusCode).intValue();
            }
        }
        return 0;
    }

    private String extractThreatLevel(Map<String, Object> doc) {
        if (doc.get("security") instanceof Map) {
            Map<String, Object> security = (Map<String, Object>) doc.get("security");
            return (String) security.get("threat_level");
        }
        return null;
    }

    private String extractEventResult(Map<String, Object> doc) {
        if (doc.get("event") instanceof Map) {
            Map<String, Object> event = (Map<String, Object>) doc.get("event");
            return (String) event.get("result");
        }
        return null;
    }

    private String extractErrorSeverity(Map<String, Object> doc) {
        if (doc.get("error") instanceof Map) {
            Map<String, Object> error = (Map<String, Object>) doc.get("error");
            return (String) error.get("severity");
        }
        return null;
    }

    private boolean hasError(Map<String, Object> doc) {
        return (doc.get("error") instanceof Map) || doc.containsKey("stack_trace");
    }

    /**
     * ì—ëŸ¬ ì‘ë‹µ ìƒì„±
     * @param errorMessage ì—ëŸ¬ ë©”ì‹œì§€
     * @return Map
     */
    private Map<String, Object> createErrorResponse(String errorMessage) {
        Map<String, Object> errorResponse = new HashMap<>();
        errorResponse.put("total", 0);
        errorResponse.put("logs", Collections.emptyList());
        errorResponse.put("error", errorMessage);
        return errorResponse;
    }

    private Double getAggregationValue(SearchResponse<Void> response, String aggName) {
        try {
            if (response.aggregations() != null && response.aggregations().get(aggName) != null) {
                co.elastic.clients.elasticsearch._types.aggregations.Aggregate agg =
                        response.aggregations().get(aggName);

                if (agg.isAvg()) {
                    return agg.avg().value();
                } else if (agg.isMax()) {
                    return agg.max().value();
                } else if (agg.isSum()) {
                    return agg.sum().value();
                }
            }
        } catch (Exception e) {
            log.warn("Failed to get aggregation value: {}", aggName);
        }
        return 0.0;
    }

    private Double getBucketAggregationValue(
            co.elastic.clients.elasticsearch._types.aggregations.DateHistogramBucket bucket,
            String aggName) {
        try {
            if (bucket.aggregations() != null && bucket.aggregations().get(aggName) != null) {
                co.elastic.clients.elasticsearch._types.aggregations.Aggregate agg =
                        bucket.aggregations().get(aggName);

                if (agg.isAvg()) {
                    return agg.avg().value();
                }
            }
        } catch (Exception e) {
            log.warn("Failed to get bucket aggregation value: {}", aggName);
        }
        return 0.0;
    }
}