package com.study.monitoring.studymonitoring.service.impl;

import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.elasticsearch._types.SortOrder;
import co.elastic.clients.elasticsearch._types.aggregations.Aggregation;
import co.elastic.clients.elasticsearch._types.aggregations.AverageAggregation;
import co.elastic.clients.elasticsearch._types.aggregations.FieldDateMath;
import co.elastic.clients.elasticsearch._types.aggregations.StringTermsBucket;
import co.elastic.clients.elasticsearch._types.query_dsl.BoolQuery;
import co.elastic.clients.elasticsearch._types.query_dsl.Query;
import co.elastic.clients.elasticsearch.core.SearchResponse;
import co.elastic.clients.elasticsearch.core.search.Hit;
import com.study.monitoring.studymonitoring.model.dto.response.PageResponseDTO;
import com.study.monitoring.studymonitoring.service.ElasticsearchService;
import com.study.monitoring.studymonitoring.util.ElasticsearchQueryUtil;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.time.Instant;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.time.temporal.ChronoUnit;
import java.util.*;
import java.util.stream.Collectors;

/**
 * Elasticsearch ÏÑúÎπÑÏä§ Íµ¨ÌòÑ
 * Ï£ºÏöî Í∞úÏÑ†ÏÇ¨Ìï≠:
 * 1. ElasticsearchQueryUtil ÌôúÏö©
 * 2. ÏóêÎü¨ Ï≤òÎ¶¨ Í∞ïÌôî
 * 3. null ÏïàÏ†ÑÏÑ± Í∞úÏÑ†
 * 4. Î°úÍπÖ Ï∂îÍ∞Ä
 */
@Slf4j
@Service
@RequiredArgsConstructor
public class ElasticsearchServiceImpl implements ElasticsearchService {

    private final ElasticsearchClient elasticsearchClient;
    private static final DateTimeFormatter FORMATTER = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");

    @Override
    public Map<String, Object> searchLogs(
            String indexPattern,
            String keyword,
            String logLevel,
            LocalDateTime startDate,
            LocalDateTime endDate,
            int from,
            int size)
    {
        try {
            log.debug("Searching logs: index={}, keyword={}, logLevel={}, startDate={}, endDate={}, from={}, size={}",
                    indexPattern, keyword, logLevel, startDate, endDate, from, size);

            // 1. Bool ÏøºÎ¶¨ ÎπåÎìú
            BoolQuery.Builder boolQuery = new BoolQuery.Builder();

            // ÌÇ§ÏõåÎìú Í≤ÄÏÉâ (Full-text Search)
            if (keyword != null && !keyword.isEmpty()) {
                boolQuery.must(ElasticsearchQueryUtil.buildMultiFieldSearchQuery(keyword));
            }

            // Î°úÍ∑∏ Î†àÎ≤® ÌïÑÌÑ∞
            if (logLevel != null && !logLevel.isEmpty()) {
               boolQuery.must(ElasticsearchQueryUtil.buildLogLevelQuery(logLevel));
            }

            // ÎÇ†Ïßú Î≤îÏúÑ ÌïÑÌÑ∞
            if (startDate != null && endDate != null) {
                boolQuery.must(ElasticsearchQueryUtil.buildDateRangeQuery(startDate, endDate));
            } else if (startDate != null) {
                // ÏãúÏûë ÎÇ†ÏßúÎßå ÏûàÎäî Í≤ΩÏö∞ (Ïù¥ÌõÑ Î™®Îì† Î°úÍ∑∏)
                boolQuery.must(ElasticsearchQueryUtil.buildDateRangeQueryFrom(startDate));
            } else if (endDate != null) {
                // Ï¢ÖÎ£å ÎÇ†ÏßúÎßå ÏûàÎäî Í≤ΩÏö∞ (Ïù¥Ï†Ñ Î™®Îì† Î°úÍ∑∏)
                boolQuery.must(ElasticsearchQueryUtil.buildDateRangeQueryTo(endDate));
            }

            // 2. Elasticsearch Í≤ÄÏÉâ Ïã§Ìñâ
            SearchResponse<Map> response = elasticsearchClient.search(
                    s -> s.index(indexPattern)
                            .from(from)
                            .size(size)
                            .query(boolQuery.build()._toQuery())
                            .sort(so -> so.field(f -> f.field("@timestamp").order(SortOrder.Desc))),
                    Map.class
            );

            // 3. Í≤∞Í≥º Î≥ÄÌôò
            List<Map<String, Object>> logs = response.hits().hits().stream()
                    .map(this::convertHitToMap)
                    .collect(Collectors.toList());

            // 4. ÏùëÎãµ Íµ¨ÏÑ±
            Map<String, Object> result = new HashMap<>();
            result.put("total", response.hits().total() != null ? response.hits().total().value() : 0);
            result.put("logs", logs);
            result.put("took", response.took());

            log.debug("Found {} logs", result.get("total"));
            return result;

        } catch (Exception e) {
            log.error("Failed to search logs: indexPattern={}", indexPattern, e);
            return createErrorResponse("Î°úÍ∑∏ Í≤ÄÏÉâ Ïã§Ìå®: " + e.getMessage());
        }
    }

    @Override
    public Map<String, Long> countByLogLevel(String indexPattern) {
        try {
            log.debug("Counting logs by level: index={}", indexPattern);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0)
                            .aggregations("by_log_level", Aggregation.of(a -> a
                                    .terms(t -> t
                                            .field("log_level.keyword").size(10)))), Void.class
            );
            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_log_level") != null) {
                response.aggregations().get("by_log_level").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()));
            }
            log.debug("Log level counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by log level: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public List<Map<String, Object>> getRecentErrors(int limit) {
        try {
            log.debug("Fetching recent errors: limit={}", limit);

            SearchResponse<Map> response = elasticsearchClient.search(s -> s
                            // [ÏàòÏ†ï 1] Ïù∏Îç±Ïä§ Ïù¥Î¶Ñ Î≥ÄÍ≤Ω: Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäî 'application-logs-*' ÏÇ¨Ïö©
                            .index("application-logs-*")
                            .size(limit)
                            // [ÏàòÏ†ï 2] ÏøºÎ¶¨ ÌïÑÎìú Î≥ÄÍ≤Ω: 'level' -> 'log_level.keyword' (Ï†úÍ≥µÎêú Îß§Ìïë Í∏∞Ï§Ä)
                            .query(q -> q
                                    .term(t -> t
                                            .field("log_level.keyword") // Îß§ÌïëÏóê Ï†ïÏùòÎêú keyword ÌïÑÎìú ÏÇ¨Ïö©
                                            .value("ERROR")
                                    )
                            )
                            // [ÏàòÏ†ï 3] Ï†ïÎ†¨ Í∏∞Ï§Ä
                            .sort(so -> so.field(f -> f.field("@timestamp").order(SortOrder.Desc))),
                    Map.class
            );

            List<Map<String, Object>> errors = response.hits().hits().stream()
                    .map(this::convertHitToMap)
                    .collect(Collectors.toList());

            log.debug("Found {} recent errors", errors.size());
            return errors;
        } catch (Exception e) {
            log.error("Failed to get recent errors", e);
            return Collections.emptyList();
        }
    }

    // ÏãúÍ∞ÑÎåÄ Î≥Ñ Î∂ÑÌè¨ - Application Logs
    @Override
    public List<Map<String, Object>> getLogDistributionByTime(
            String indexPattern,
            LocalDateTime startTime,
            LocalDateTime endTime,
            String timePeriod,
            String logLevel)
    {
        try {
            log.info("Querying log distribution: {} ~ {}, period={}, logLevel={}",
                    startTime, endTime, timePeriod, logLevel);

            String interval = calculateInterval(timePeriod);
            BoolQuery.Builder boolQuery = new BoolQuery.Builder();
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(startTime, endTime);
            boolQuery.must(timeRangeQuery);

            if (logLevel != null && !logLevel.isEmpty() && !"undefined".equals(logLevel)) {
                boolQuery.must(ElasticsearchQueryUtil.buildLogLevelQuery(logLevel));
            }

            // ‚úÖ epoch millisecondsÎ°ú Î≥ÄÌôò
            long startEpochMs = startTime.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = endTime.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern)
                            .size(0)
                            .query(boolQuery.build()._toQuery())
                            .aggregations("logs_over_time",
                                    Aggregation.of(a -> a
                                            .dateHistogram(dh -> dh
                                                    .field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            // ‚úÖ ÏàòÏ†ï: epoch milliseconds ÏÇ¨Ïö©
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                            )
                                    )
                            ),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("logs_over_time") != null) {
                response.aggregations().get("logs_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("count", bucket.docCount());
                            distribution.add(entry);
                        });
            }
            log.info("Distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get log distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public Map<String, Long> countByHttpMethod(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by HTTP method: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(s ->
                            s.index(indexPattern).size(0)
                                    .query(timeRangeQuery)
                                    .aggregations("by_method", Aggregation.of(a ->
                                            // ‚úÖ ÏàòÏ†ï: http.method -> http.method.keyword (nested object ÎÇ¥ keyword ÌïÑÎìú)
                                            a.terms(t -> t.field("http.method.keyword").size(10))
                                    )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_method") != null) {
                response.aggregations().get("by_method").sterms().buckets().array()
                        .forEach(bucket -> counts.put(bucket.key().stringValue(), bucket.docCount()));
            }
            log.debug("HTTP method counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by HTTP method: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByStatusCode(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by status code: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(s ->
                            s.index(indexPattern).size(0)   // ÌÜµÍ≥Ñ Í≤∞Í≥º(ÏßëÍ≥Ñ)Îßå ÌïÑÏöîÌï† Îïå ÏÇ¨Ïö©
                            .query(timeRangeQuery).aggregations("by_status", Aggregation.of(a ->
                                    a.terms(t ->
                                    t.field("http.status_code").size(20))   // ÏÉÅÏúÑ 20Í∞úÏùò ÏÉÅÌÉú ÏΩîÎìúÎßå Í∞ÄÏ†∏Ïò§Í≤†Îã§.
                            )),
                    Void.class
            );
            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_status") != null) {
                response.aggregations().get("by_status").lterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                String.valueOf(bucket.key()),   // ÏÉÅÌÉú ÏΩîÎìú Í∞í(Ïòà: 200)
                                bucket.docCount()));            // ÏÉÅÌÉú ÏΩîÎìúÍ∞Ä ÎÇòÌÉÄÎÇú ÌöüÏàò
            }
            log.debug("Status code counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by status code: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Double getAverageResponseTime(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting average response time: index={}, start={}, end={}", indexPattern, start, end);
            Query timeResponseQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end); // ÌäπÏ†ï ÏãúÍ∞Ñ Î≤îÏúÑÏóê Ìï¥ÎãπÌïòÎäî Îç∞Ïù¥ÌÑ∞Îßå ÌïÑÌÑ∞ÎßÅ
            SearchResponse<Void> response = elasticsearchClient.search(s ->
                            s.index(indexPattern).size(0).query(timeResponseQuery)
                                    .aggregations("avg_response_time", Aggregation.of(a ->
                                            a.avg(avg -> avg.field("http.response_time_ms")))),
                    Void.class
            );

            if (response.aggregations() != null && response.aggregations().get("avg_response_time") != null) {
                Double avgValue = response.aggregations().get("avg_response_time").avg().value();
                log.debug("Average response time: {} ms", avgValue);
                return avgValue != null ? avgValue : 0.0;
            }

            return 0.0;
        } catch (Exception e) {
            log.error("Failed to get average response time indexPattern={}", indexPattern, e);
            return 0.0;
        }
    }

    // ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÌè¨ - Access Logs
    @Override
    public List<Map<String, Object>> getAccessLogDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying access log distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // ‚úÖ epoch millisecondsÎ°ú Î≥ÄÌôò
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("access_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("avg_response_time", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("http.response_time_ms")))
                                    ).aggregations("error_count", Aggregation.of(sub ->
                                            sub.filter(f -> f.range(
                                                    r -> r.field("http.status_code")
                                                            .gte(co.elastic.clients.json.JsonData.of(500)))
                                            )
                                    ))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("access_over_time") != null) {
                response.aggregations().get("access_over_time").dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("requestCount", bucket.docCount());

                            Double avgResponseTime = bucket.aggregations().get("avg_response_time").avg().value();
                            entry.put("avgResponseTime", avgResponseTime != null ? avgResponseTime : 0.0);

                            Long errorCount = bucket.aggregations().get("error_count").filter().docCount();
                            entry.put("errorCount", errorCount);
                            distribution.add(entry);
                        });
            }
            log.info("Access log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get access log distribution", e);
            return Collections.emptyList();
        }
    }

    // ============================================
    // üîÑ error-logs ÌÜµÍ≥ÑÏö© Î©îÏÑúÎìú
    // ============================================
    @Override
    public Map<String, Long> countByErrorType(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by error type: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_error_type", Aggregation.of(
                                    // ‚úÖ ÏàòÏ†ï: error.type -> error.type.keyword
                                    a -> a.terms(t -> t.field("error.type.keyword").size(20))
                            )), Void.class
            );
            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_error_type") != null) {
                response.aggregations().get("by_error_type").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Error type counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by error type: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countBySeverity(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by severity: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern)
                            .size(0).query(timeRangeQuery)
                            .aggregations("by_severity", Aggregation.of(
                                    // ‚úÖ ÏàòÏ†ï: error.severity -> error.severity.keyword
                                    a -> a.terms(t -> t.field("error.severity.keyword").size(10))
                            )), Void.class
            );
            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_severity") != null) {
                response.aggregations().get("by_severity").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Severity counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by severity: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    // ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÌè¨ - Error Logs
    @Override
    public List<Map<String, Object>> getErrorLogDistributionByTime(String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying error log distribution: {} ~ {}, period={}", start, end, timePeriod);
            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("errors_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                    ).aggregations("error_type_breakdown", Aggregation.of(
                                            // ‚úÖ ÏàòÏ†ï: error.type -> error.type.keyword
                                            sub -> sub.terms(t -> t.field("error.type.keyword").size(5))
                                    ))
                            )),
                    Void.class
            );
            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("errors_over_time") != null) {
                response.aggregations().get("errors_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("errorCount", bucket.docCount());

                            // ÏóêÎü¨ ÌÉÄÏûÖÎ≥Ñ Î∂ÑÌè¨
                            Map<String, Long> errorTypeBreakdown = new HashMap<>();
                            bucket.aggregations().get("error_type_breakdown")
                                    .sterms().buckets().array()
                                    .forEach(typeBucket -> errorTypeBreakdown.put(
                                            typeBucket.key().stringValue(),
                                            typeBucket.docCount()));
                            entry.put("errorTypeBreakdown", errorTypeBreakdown);
                            distribution.add(entry);
                        });
            }
            log.info("Error log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get error log distribution: indexPattern={}", indexPattern, e);
            return Collections.emptyList();
        }
    }

    // ============================================
    // üîÑ performance-metrics ÌÜµÍ≥ÑÏö© Î©îÏÑúÎìú
    // ============================================
    @Override
    public Map<String, Double> getSystemMetricsAggregation(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting system metrics aggregation: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("avg_cpu", Aggregation.of(
                                    a -> a.avg(avg -> avg.field("system.cpu_usage"))
                            ))
                            .aggregations("max_cpu", Aggregation.of(
                                    a -> a.max(max -> max.field("system.cpu_usage"))
                            ))
                            .aggregations("avg_memory", Aggregation.of(
                                    a -> a.avg(avg -> avg.field("system.memory_usage"))
                            ))
                            .aggregations("max_memory", Aggregation.of(
                                    a -> a.max(max -> max.field("system.memory_usage"))
                            ))
                            .aggregations("avg_disk", Aggregation.of(
                                    a -> a.avg(avg -> avg.field("system.disk_usage"))
                            )), Void.class
            );
            Map<String, Double> metrics = new HashMap<>();
            if (response.aggregations() != null) {
                metrics.put("avg_cpu", getAggregationValue(response, "avg_cpu"));
                metrics.put("max_cpu", getAggregationValue(response, "max_cpu"));
                metrics.put("avg_memory", getAggregationValue(response, "avg_memory"));
                metrics.put("max_memory", getAggregationValue(response, "max_memory"));
                metrics.put("avg_disk", getAggregationValue(response, "avg_disk"));
            }
            log.debug("System metrics: {}", metrics);
            return metrics;
        } catch (Exception e) {
            log.error("Failed to get system metrics aggregation: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Double> getJvmMetricsAggregation(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting JVM metrics aggregation: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
        s -> s.index(indexPattern).size(0).query(timeRangeQuery).aggregations("avg_heap", Aggregation.of(
                a -> a.avg(avg -> avg.field("jvm.heap_used")))).aggregations("max_heap", Aggregation.of(
                    a -> a.max(max -> max.field("jvm.heap_used")))).aggregations("total_gc_count", Aggregation.of(
                        a -> a.sum(sum -> sum.field("jvm.gc_count")))).aggregations("total_gc_time", Aggregation.of(
                            a -> a.sum(sum -> sum.field("jvm.gc_time")))).aggregations("avg_thread_count", Aggregation.of(
                                a -> a.avg(avg -> avg.field("jvm.thread_count")))),
                    Void.class
            );
            Map<String, Double> metrics = new HashMap<>();
            if (response.aggregations() != null) {
                metrics.put("avg_heap", getAggregationValue(response, "avg_heap"));
                metrics.put("max_heap", getAggregationValue(response, "max_heap"));
                metrics.put("total_gc_count", getAggregationValue(response, "total_gc_count"));
                metrics.put("total_gc_time", getAggregationValue(response, "total_gc_time"));
                metrics.put("avg_thread_count", getAggregationValue(response, "avg_thread_count"));
            }
            log.debug("JVM metrics: {}", metrics);
            return metrics;
        } catch (Exception e) {
            log.error("Failed to get JVM metrics aggregation: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    // ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÌè¨ - Performance Metrics
    @Override
    public List<Map<String, Object>> getPerformanceMetricsDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying performance metrics distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // ‚úÖ epoch millisecondsÎ°ú Î≥ÄÌôò
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("metrics_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("avg_cpu_usage", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("system.cpu_usage")))
                                    ).aggregations("avg_memory_usage", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("system.memory_usage")))
                                    ).aggregations("avg_heap_usage", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("jvm.heap_used"))))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("metrics_over_time") != null) {
                response.aggregations().get("metrics_over_time").dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("cpuUsage", getBucketAggregationValue(bucket, "avg_cpu_usage"));
                            entry.put("memoryUsage", getBucketAggregationValue(bucket, "avg_memory_usage"));
                            entry.put("heapUsage", getBucketAggregationValue(bucket, "avg_heap_usage"));
                            distribution.add(entry);
                        });
            }
            log.info("Performance metrics distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get performance metrics distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public Map<String, Long> countByOperation(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by operation: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_operation", Aggregation.of(
                                    // ‚úÖ ÏàòÏ†ï: operation -> operation.keyword
                                    a -> a.terms(t -> t.field("operation.keyword").size(10))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_operation") != null) {
                response.aggregations().get("by_operation").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Operation counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by operation: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByTable(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by table: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_table", Aggregation.of(
                                    // ‚úÖ ÏàòÏ†ï: table -> table.keyword
                                    a -> a.terms(t -> t.field("table.keyword").size(20))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_table") != null) {
                response.aggregations().get("by_table").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount())
                        );
            }
            log.debug("Table counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by table: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Object> getQueryPerformanceStats(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting query performance stats: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("avg_duration", Aggregation.of(
                                    a -> a.avg(avg -> avg.field("duration_ms"))
                            ))
                            .aggregations("max_duration", Aggregation.of(a -> a
                                    .max(max -> max.field("duration_ms"))
                            ))
                            .aggregations("slow_queries", Aggregation.of(a -> a
                                    .filter(f -> f
                                            .range(r -> r
                                                    .field("duration_ms")
                                                    .gte(co.elastic.clients.json.JsonData.of(1000))
                                            )))), Void.class
            );
            Map<String, Object> stats = new HashMap<>();
            if (response.aggregations() != null) {
                stats.put("avgDuration", getAggregationValue(response, "avg_duration"));
                stats.put("maxDuration", getAggregationValue(response, "max_duration"));
                stats.put("slowQueryCount", response.aggregations().get("slow_queries").filter().docCount());
                long totalCount = response.hits().total() != null ? response.hits().total().value() : 0;
                stats.put("totalQueryCount", totalCount);
            }
            log.debug("Query performance stats: {}", stats);
            return stats;
        } catch (Exception e) {
            log.error("Failed to get query performance stats: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    // ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÌè¨ - Database Logs
    @Override
    public List<Map<String, Object>> getDatabaseLogDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying database log distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // ‚úÖ epoch millisecondsÎ°ú Î≥ÄÌôò
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("db_logs_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("avg_duration", Aggregation.of(
                                            sub -> sub.avg(avg -> avg.field("query.duration_ms")))
                                    ).aggregations("slow_query_count", Aggregation.of(
                                            sub -> sub.filter(f -> f.range(
                                                    r -> r.field("query.duration_ms")
                                                            .gte(co.elastic.clients.json.JsonData.of(1000))))))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("db_logs_over_time") != null) {
                response.aggregations().get("db_logs_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("queryCount", bucket.docCount());
                            entry.put("avgDuration", getBucketAggregationValue(bucket, "avg_duration"));
                            entry.put("slowQueryCount", bucket.aggregations().get("slow_query_count").filter().docCount());
                            distribution.add(entry);
                        });
            }
            log.info("Database log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get database log distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public Map<String, Long> countByEventAction(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by event action: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_action", Aggregation.of(
                                    // ‚úÖ ÏàòÏ†ï: event.action.keyword (nested Íµ¨Ï°∞)
                                    a -> a.terms(t -> t.field("event.action.keyword").size(20))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_action") != null) {
                response.aggregations().get("by_action").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Event action counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to get event action", e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByCategory(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by category: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_category", a -> a.terms(
                                    // ‚úÖ ÏàòÏ†ï: event.category.keyword
                                    t -> t.field("event.category.keyword").size(10)
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_category") != null) {
                List<StringTermsBucket> buckets = response.aggregations().get("by_category").sterms().buckets().array();
                for (StringTermsBucket bucket : buckets) {
                    counts.put(bucket.key().stringValue(), bucket.docCount());
                }
            }
            log.debug("Category counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by category", e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByEventResult(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by event result: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_result", Aggregation.of(
                                    // ‚úÖ ÏàòÏ†ï: event.result.keyword
                                    a -> a.terms(t -> t.field("event.result.keyword").size(5))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_result") != null) {
                response.aggregations().get("by_result").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Event result counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by event result", e);
            return Collections.emptyMap();
        }
    }

    // ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÌè¨ - Audit Logs
    @Override
    public List<Map<String, Object>> getAuditLogDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying audit log distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // ‚úÖ epoch millisecondsÎ°ú Î≥ÄÌôò
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("audit_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("success_count", Aggregation.of(
                                            sub -> sub.filter(f -> f.term(t -> t.field("event.result.keyword").value("success"))))
                                    ).aggregations("failure_count", Aggregation.of(
                                            sub -> sub.filter(f -> f.term(t -> t.field("event.result.keyword").value("failure")))))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("audit_over_time") != null) {
                response.aggregations().get("audit_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("totalEvents", bucket.docCount());

                            Long successCount = bucket.aggregations().get("success_count").filter().docCount();
                            Long failureCount = bucket.aggregations().get("failure_count").filter().docCount();
                            entry.put("successEvents", successCount);
                            entry.put("failureEvents", failureCount);
                            distribution.add(entry);
                        });
            }
            log.info("Audit log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get audit log distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public Map<String, Long> countByThreatLevel(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by threat level: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_threat_level", Aggregation.of(
                                    // ‚úÖ ÏàòÏ†ï: Logstash Ïû¨Íµ¨ÏÑ± ÌõÑ nested Íµ¨Ï°∞ ÏÇ¨Ïö©
                                    a -> a.terms(t -> t.field("security.threat_level.keyword").size(10))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_threat_level") != null) {
                response.aggregations().get("by_threat_level").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Threat level counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by threat level: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> countByAttackType(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Counting by attack type: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("by_attack_type", Aggregation.of(
                                    // ‚úÖ ÏàòÏ†ï: attack.type.keyword (nested Íµ¨Ï°∞)
                                    a -> a.terms(t -> t.field("attack.type.keyword").size(20))
                            )),
                    Void.class
            );

            Map<String, Long> counts = new HashMap<>();
            if (response.aggregations() != null && response.aggregations().get("by_attack_type") != null) {
                response.aggregations().get("by_attack_type").sterms().buckets().array()
                        .forEach(bucket -> counts.put(
                                bucket.key().stringValue(),
                                bucket.docCount()
                        ));
            }
            log.debug("Attack type counts: {}", counts);
            return counts;
        } catch (Exception e) {
            log.error("Failed to count by attack type: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    @Override
    public Map<String, Long> getBlockStatistics(String indexPattern, LocalDateTime start, LocalDateTime end) {
        try {
            log.debug("Getting block statistics: index={}, start={}, end={}", indexPattern, start, end);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);
            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("blocked_attacks", Aggregation.of(
                                    a -> a.filter(
                                            f -> f.term(
                                                    t -> t.field("blocked").value(true)))))
                            .aggregations("allowed_attacks", Aggregation.of(
                                    a -> a.filter(f ->
                                            f.term(t -> t.field("blocked").value(false))))), Void.class
            );
            Map<String, Long> stats = new HashMap<>();
            if (response.aggregations() != null) {
                long totalAttacks = response.hits().total() != null ? response.hits().total().value() : 0;
                long blockedAttacks = response.aggregations().get("blocked_attacks").filter().docCount();
                long allowedAttacks = response.aggregations().get("allowed_attacks").filter().docCount();

                stats.put("totalAttacks", totalAttacks);
                stats.put("blockedAttacks", blockedAttacks);
                stats.put("allowedAttacks", allowedAttacks);
            }
            log.debug("Block statistics: {}", stats);
            return stats;
        } catch (Exception e) {
            log.error("Failed to get block statistics: indexPattern={}", indexPattern, e);
            return Collections.emptyMap();
        }
    }

    // ÏãúÍ∞ÑÎåÄÎ≥Ñ Î∂ÑÌè¨ - Security Logs
    @Override
    public List<Map<String, Object>> getSecurityLogDistributionByTime(
            String indexPattern, LocalDateTime start, LocalDateTime end, String timePeriod) {
        try {
            log.info("Querying security log distribution: {} ~ {}, period={}", start, end, timePeriod);

            String interval = calculateInterval(timePeriod);
            Query timeRangeQuery = ElasticsearchQueryUtil.buildDateRangeQuery(start, end);

            // ‚úÖ epoch millisecondsÎ°ú Î≥ÄÌôò
            long startEpochMs = start.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();
            long endEpochMs = end.atZone(ZoneId.of("Asia/Seoul")).toInstant().toEpochMilli();

            SearchResponse<Void> response = elasticsearchClient.search(
                    s -> s.index(indexPattern).size(0).query(timeRangeQuery)
                            .aggregations("security_over_time", Aggregation.of(
                                    a -> a.dateHistogram(
                                            dh -> dh.field("@timestamp")
                                                    .fixedInterval(fi -> fi.time(interval))
                                                    .timeZone("Asia/Seoul")
                                                    .format("yyyy-MM-dd HH:mm:ss")
                                                    .minDocCount(0)
                                                    .extendedBounds(b -> b
                                                            .min(FieldDateMath.of(f -> f.value((double) startEpochMs)))
                                                            .max(FieldDateMath.of(f -> f.value((double) endEpochMs)))
                                                    )
                                    ).aggregations("blocked_count", Aggregation.of(
                                            sub -> sub.filter(f -> f.term(t -> t.field("blocked").value(true))))
                                    ).aggregations("threat_level_breakdown", Aggregation.of(
                                            sub -> sub.terms(t -> t.field("security.threat_level.keyword").size(5))))
                            )),
                    Void.class
            );

            List<Map<String, Object>> distribution = new ArrayList<>();
            if (response.aggregations() != null && response.aggregations().get("security_over_time") != null) {
                response.aggregations().get("security_over_time")
                        .dateHistogram().buckets().array()
                        .forEach(bucket -> {
                            Map<String, Object> entry = new HashMap<>();
                            entry.put("timestamp", bucket.keyAsString());
                            entry.put("attackCount", bucket.docCount());

                            Long blockedCount = bucket.aggregations().get("blocked_count").filter().docCount();
                            entry.put("blockedCount", blockedCount);

                            Map<String, Long> threatLevelBreakdown = new HashMap<>();
                            bucket.aggregations().get("threat_level_breakdown")
                                    .sterms().buckets().array()
                                    .forEach(threatBucket -> threatLevelBreakdown.put(
                                            threatBucket.key().stringValue(),
                                            threatBucket.docCount()
                                    ));
                            entry.put("threatLevelBreakdown", threatLevelBreakdown);
                            distribution.add(entry);
                        });
            }
            log.info("Security log distribution result: {} time buckets", distribution.size());
            return distribution;
        } catch (Exception e) {
            log.error("Failed to get security log distribution", e);
            return Collections.emptyList();
        }
    }

    @Override
    public PageResponseDTO<Map<String, Object>> searchErrorLogs(String type, int page, int size) {
        String indexName;

        // ÌÉ≠Ïóê Îî∞Îùº Ïù∏Îç±Ïä§ Í≤∞Ï†ï
        if ("SYSTEM".equalsIgnoreCase(type)) {
            indexName = "error-logs-*"; // ÏóêÎü¨ Ï†ÑÏö© Î°úÍ∑∏ Ïù∏Îç±Ïä§
        } else {
            indexName = "application-logs-*"; // ÏùºÎ∞ò Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Î°úÍ∑∏ Ïù∏Îç±Ïä§
        }

        int currentPage = Math.max(1, page);
        int from = (currentPage - 1) * size;

        try {
            SearchResponse<Map> response = elasticsearchClient.search(s -> s
                            .index(indexName)
                            .from(from)
                            .size(size)
                            .query(q -> {
                                if ("SYSTEM".equalsIgnoreCase(type)) {
                                    // error-logs-* Ïù∏Îç±Ïä§Îäî Î™®Îì† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóêÎü¨Ïù¥ÎØÄÎ°ú Î≥ÑÎèÑ ÌïÑÌÑ∞ ÏóÜÏù¥ Ï†ÑÏ≤¥ Ï°∞Ìöå
                                    // (ÌïÑÏöîÌïòÎã§Î©¥ severityÍ∞Ä CRITICAL/FATALÏù∏ Í≤ÉÎßå ÌïÑÌÑ∞ÎßÅ Í∞ÄÎä•)
                                    return q.matchAll(m -> m);
                                } else {
                                    // application-logs-* Ïù∏Îç±Ïä§Îäî log_levelÏù¥ ERRORÏù∏ Í≤ÉÎßå Ï°∞Ìöå
                                    return q.term(t -> t.field("log_level.keyword").value("ERROR"));
                                }
                            })
                            .sort(so -> so.field(f -> f.field("@timestamp").order(SortOrder.Desc))),
                    Map.class
            );

            List<Map<String, Object>> content = response.hits().hits().stream()
                    .map(this::convertHitToMap)
                    .collect(Collectors.toList());

            long totalElements = response.hits().total() != null ? response.hits().total().value() : 0;
            int totalPages = (int) Math.ceil((double) totalElements / size);

            return PageResponseDTO.<Map<String, Object>>builder()
                    .content(content)
                    .totalElements(totalElements)
                    .totalPages(totalPages)
                    .currentPage(currentPage)
                    .size(size)
                    .build();

        } catch (Exception e) {
            log.error("Error searching logs type={}", type, e);
            return PageResponseDTO.<Map<String, Object>>builder()
                    .content(Collections.emptyList())
                    .build();
        }
    }

    // timePeriod ‚Üí Elasticsearch interval Î≥ÄÌôò
    private String calculateInterval(String timePeriod) {
        return switch (timePeriod.toUpperCase()) {
            case "MINUTE" -> "1m";
            case "HOUR" -> "1h";
            case "DAY" -> "1d";
            case "WEEK" -> "7d";
            case "MONTH" -> "30d";
            default -> "1h";
        };
    }

    /**
     * Elasticsearch HitÏùÑ MapÏúºÎ°ú Î≥ÄÌôò
     *
     * @param hit Hit Í∞ùÏ≤¥
     * @return Map
     */
    private Map<String, Object> convertHitToMap(Hit<Map> hit) {
        Map<String, Object> result = new HashMap<>();

        // Î¨∏ÏÑú ID Î∞è Ïù∏Îç±Ïä§ Ï∂îÍ∞Ä
        result.put("_id", hit.id());
        result.put("_index", hit.index());

        // ÏÜåÏä§ Îç∞Ïù¥ÌÑ∞
        Map<String, Object> source = hit.source();
        if (source == null) {
            return result;
        }

        // Í≥µÌÜµ ÌïÑÎìú: @timestamp, application
        result.put("@timestamp", source.get("@timestamp"));
        result.put("application", source.get("application"));

        // ‚úÖ Ïù∏Îç±Ïä§ ÌÉÄÏûÖÎ≥Ñ ÌïÑÎìú Îß§Ìïë
        String indexName = hit.index();

        if (indexName.startsWith("application-logs")) {
            // application-logs: ÌëúÏ§Ä Î°úÍ∑∏ ÌïÑÎìú
            result.put("log_level", source.get("log_level"));
            result.put("logger_name", source.get("logger_name"));
            result.put("message", source.get("message"));
            result.put("stack_trace", source.get("stack_trace"));
            result.put("thread_name", source.get("thread_name"));

        } else if (indexName.startsWith("access-logs")) {
            // access-logs: HTTP Ï†ëÍ∑º Î°úÍ∑∏
            Map<String, Object> http = (Map<String, Object>) source.get("http");
            if (http != null) {
                result.put("log_level", "INFO"); // Í∏∞Î≥∏ Î†àÎ≤®
                result.put("logger_name", "AccessLog");

                // HTTP Ï†ïÎ≥¥Î•º Î©îÏãúÏßÄÎ°ú Íµ¨ÏÑ±
                String message = String.format("%s %s - Status: %s, Response Time: %sms",
                        http.get("method"),
                        http.get("url"),
                        http.get("status_code"),
                        http.get("response_time_ms")
                );
                result.put("message", message);

                // ÏõêÎ≥∏ HTTP Îç∞Ïù¥ÌÑ∞ÎèÑ Ìè¨Ìï®
                result.put("http", http);
            }

            Map<String, Object> client = (Map<String, Object>) source.get("client");
            if (client != null) {
                result.put("client", client);
            }

        } else if (indexName.startsWith("error-logs")) {
            // error-logs: ÏóêÎü¨ Î°úÍ∑∏
            Map<String, Object> error = (Map<String, Object>) source.get("error");
            if (error != null) {
                result.put("log_level", error.get("severity")); // severityÎ•º log_levelÎ°ú Îß§Ìïë
                result.put("logger_name", "ErrorLog");
                result.put("message", error.get("type") + ": " + error.get("message"));
                result.put("stack_trace", error.get("stack_trace"));
                result.put("error", error);
            }

            Map<String, Object> sourceInfo = (Map<String, Object>) source.get("source");
            if (sourceInfo != null) {
                result.put("source", sourceInfo);
            }

        } else if (indexName.startsWith("performance-metrics")) {
            // performance-metrics: ÏÑ±Îä• Î©îÌä∏Î¶≠ (ÏàòÏ†ïÎê®)
            result.put("log_level", "INFO");

            // 1. Î©îÏÑúÎìú Ïã§Ìñâ ÏãúÍ∞Ñ Î°úÍ∑∏Ïù∏ÏßÄ ÌôïÏù∏ (class, method, execution_time_ms ÌïÑÎìú Ï°¥Ïû¨ Ïó¨Î∂Ä)
            if (source.containsKey("method") && source.containsKey("execution_time_ms")) {
                // Logger Name: ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ ÏÇ¨Ïö© (ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í)
                Object className = source.get("class");
                result.put("logger_name", className != null ? className : "PerformanceLog");

                // Message: "Method Execution: checkLoginId - 4208ms" ÌòïÌÉúÎ°ú Í∞ÄÍ≥µ
                String message = String.format("Method Execution: %s - %sms",
                        source.get("method"),
                        source.get("execution_time_ms")
                );
                result.put("message", message);

                // ÏÉÅÏÑ∏ Îç∞Ïù¥ÌÑ∞ ÏõêÎ≥∏ÎèÑ Ìè¨Ìï® (ÌîÑÎ°†Ìä∏ÏóîÎìú Ï†ïÎ†¨/ÌïÑÌÑ∞ÎßÅÏö©)
                result.put("class", className);
                result.put("method", source.get("method"));
                result.put("execution_time_ms", source.get("execution_time_ms"));

            } else {
                // 2. ÏãúÏä§ÌÖú/JVM Î©îÌä∏Î¶≠ (Í∏∞Ï°¥ Î°úÏßÅ - ÌòπÏãú ÏãúÏä§ÌÖú Î°úÍ∑∏Í∞Ä Îì§Ïñ¥Ïò¨ Í≤ΩÏö∞Î•º ÎåÄÎπÑÌï¥ Ïú†ÏßÄ)
                result.put("logger_name", "SystemMetrics");

                Map<String, Object> system = (Map<String, Object>) source.get("system");
                Map<String, Object> jvm = (Map<String, Object>) source.get("jvm");

                StringBuilder sb = new StringBuilder("System Metrics");
                if (system != null) {
                    sb.append(String.format(" - CPU: %s%%", system.get("cpu_usage")));
                    result.put("system", system);
                }
                if (jvm != null) {
                    result.put("jvm", jvm);
                }

                // ÏõêÎ≥∏ Î©îÏãúÏßÄÍ∞Ä "Performance Data" Ï≤òÎüº Îã®ÏàúÌïòÎ©¥ ÏÉÅÏÑ∏ Ï†ïÎ≥¥Î•º, ÏïÑÎãàÎ©¥ ÏõêÎ≥∏ Î©îÏãúÏßÄÎ•º ÏÇ¨Ïö©
                String originalMsg = (String) source.get("message");
                if (originalMsg != null && !originalMsg.equals("Performance Data")) {
                    result.put("message", originalMsg);
                } else {
                    result.put("message", sb.toString());
                }
            }

        } else if (indexName.startsWith("database-logs")) {
            // database-logs: Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÍ∑∏

            // 1. Í∏∞Ï°¥Ï≤òÎüº Íµ¨Ï°∞ÌôîÎêú ÏøºÎ¶¨ Í∞ùÏ≤¥Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
            Map<String, Object> query = (Map<String, Object>) source.get("query");

            if (query != null) {
                // [Case A] Íµ¨Ï°∞ÌôîÎêú Î°úÍ∑∏Í∞Ä Îì§Ïñ¥Ïò® Í≤ΩÏö∞ (Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ)
                result.put("log_level", "INFO");
                result.put("logger_name", "DatabaseLog");

                String message = String.format("%s - %s (Duration: %sms)",
                        source.get("operation"),
                        source.get("table"),
                        query.get("duration_ms")
                );
                result.put("message", message);
                result.put("query", query);
                result.put("stack_trace", query.get("sql"));
                result.put("operation", source.get("operation"));
                result.put("table", source.get("table"));

            } else {
                // ‚úÖ [Case B] Ïö∞Î¶¨Í∞Ä ÎßåÎì† Interceptor Î°úÍ∑∏ (ÏùºÎ∞ò ÌÖçÏä§Ìä∏ Î©îÏãúÏßÄ) Ï≤òÎ¶¨
                // Íµ¨Ï°∞ÌôîÎêú 'query' Í∞ùÏ≤¥Í∞Ä ÏóÜÎã§Î©¥, ÏõêÎ≥∏ 'message' ÌïÑÎìúÎ•º Í∑∏ÎåÄÎ°ú Í∞ÄÏ†∏ÏòµÎãàÎã§.

                // Î°úÍ∑∏ Î†àÎ≤® Í∞ÄÏ†∏Ïò§Í∏∞ (ÏóÜÏúºÎ©¥ INFO)
                Object logLevel = source.get("log_level");
                result.put("log_level", logLevel != null ? logLevel : "INFO");

                // Î°úÍ±∞ Ïù¥Î¶Ñ Í∞ÄÏ†∏Ïò§Í∏∞ (ÏóÜÏúºÎ©¥ DatabaseLog)
                Object loggerName = source.get("logger_name");
                result.put("logger_name", loggerName != null ? loggerName : "DatabaseLog");

                // ‚òÖ ÌïµÏã¨: InterceptorÍ∞Ä ÎßåÎì† "SQL: [...]" Î¨∏ÏûêÏó¥ÏùÑ Í∑∏ÎåÄÎ°ú Ï†ÑÎã¨
                result.put("message", source.get("message"));

                // Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§ Ï†ïÎ≥¥Í∞Ä ÏûàÎã§Î©¥ Ï∂îÍ∞Ä
                if (source.containsKey("stack_trace")) {
                    result.put("stack_trace", source.get("stack_trace"));
                }
            }

        } else if (indexName.startsWith("audit-logs")) {
            // audit-logs: Í∞êÏÇ¨ Î°úÍ∑∏ (ÏàòÏ†ïÎê®)

            // 1. Í∏∞Î≥∏ ÏÑ§Ï†ï (Îç∞Ïù¥ÌÑ∞Ïóê Î†àÎ≤®Ïù¥ ÏóÜÏúºÎØÄÎ°ú INFOÎ°ú Í≥†Ï†ï)
            result.put("log_level", "INFO");
            result.put("logger_name", "AuditLog");

            // 2. Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            Map<String, Object> user = (Map<String, Object>) source.get("user");
            Map<String, Object> resource = (Map<String, Object>) source.get("resource");
            String originalMessage = (String) source.get("message");

            // 3. Î©îÏãúÏßÄ Ïû¨Íµ¨ÏÑ± (ÎàÑÍ∞Ä, Î¨¥ÏóáÏùÑ ÌñàÎäîÏßÄ Î™ÖÌôïÌïòÍ≤å ÌëúÏãú)
            // Ïòà: "User registration completed by test001 (Resource: ÌÖåÏä§ÌÑ∞)"
            StringBuilder messageBuilder = new StringBuilder();
            messageBuilder.append(originalMessage != null ? originalMessage : "Audit Event");

            if (user != null && user.get("login_id") != null) {
                messageBuilder.append(" by ").append(user.get("login_id"));
            }

            if (resource != null && resource.get("name") != null) {
                messageBuilder.append(" (Resource: ").append(resource.get("name")).append(")");
            }

            result.put("message", messageBuilder.toString());

            // 4. ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Îã¥Í∏∞ (ÌîÑÎ°†Ìä∏ÏóîÎìú ÏÉÅÏÑ∏ Î™®Îã¨Ïö©)
            if (user != null) {
                result.put("user", user);
            }
            if (resource != null) {
                result.put("resource", resource);
            }

            // Í∏∞Ï°¥ 'event' Í∞ùÏ≤¥Í∞Ä ÏûàÎã§Î©¥ Í∞ôÏù¥ ÎÑ£Ïñ¥Ï§å (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
            if (source.containsKey("event")) {
                result.put("event", source.get("event"));
            }

        } else if (indexName.startsWith("security-logs")) {
            // =================================================
            // [SEC] Î≥¥Ïïà Î°úÍ∑∏ (ÏßÄÎä•Ìòï Î∂ÑÏÑù Ï†ÅÏö©)
            // =================================================

            // 1. Íµ¨Ï°∞ÌôîÎêú Î≥¥Ïïà Ïù¥Î≤§Ìä∏ Í∞ùÏ≤¥(security, attack)Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏ (WAF Îì± Ïó∞Îèô Ïãú)
            Map<String, Object> security = (Map<String, Object>) source.get("security");
            Map<String, Object> attack = (Map<String, Object>) source.get("attack");

            if (security != null && attack != null) {
                // [Case A] Íµ¨Ï°∞ÌôîÎêú ÏúÑÌòë Î°úÍ∑∏ Ï≤òÎ¶¨
                String threatLevel = (String) security.get("threat_level");

                // Threat Level -> Log Level Îß§Ìïë
                String logLevel;
                switch (threatLevel != null ? threatLevel.toLowerCase() : "low") {
                    case "critical": logLevel = "FATAL"; break;
                    case "high":     logLevel = "ERROR"; break;
                    case "medium":   logLevel = "WARN";  break;
                    default:         logLevel = "INFO";  break;
                }

                result.put("log_level", logLevel);
                result.put("logger_name", "SecurityEvent");
                result.put("message", String.format("[%s] Security Alert: %s (Blocked: %s)",
                        threatLevel, attack.get("type"), source.get("blocked")));

                result.put("security", security);
                result.put("attack", attack);

            } else {
                // [Case B] ÏùºÎ∞ò Spring Security ÌÖçÏä§Ìä∏ Î°úÍ∑∏ Î∂ÑÏÑù

                String rawMessage = (String) source.get("message");
                Object originalLevelObj = source.get("level");
                String level = originalLevelObj != null ? originalLevelObj.toString() : "INFO";

                // Î°úÍ±∞ Ïù¥Î¶Ñ Ï†ïÎ¶¨ (Ìå®ÌÇ§ÏßÄÎ™Ö Îã®Ï∂ï)
                String loggerName = "SecurityLog";
                if (source.get("logger") != null) {
                    String fullLogger = source.get("logger").toString();
                    loggerName = fullLogger.contains(".")
                            ? fullLogger.substring(fullLogger.lastIndexOf(".") + 1)
                            : fullLogger;
                }

                // --- üîç Î©îÏãúÏßÄ Î∂ÑÏÑù Î∞è Î†àÎ≤®/Ïú†Ìòï Ïû¨Ï†ïÏùò ---
                String securityType = "General Event";

                if (rawMessage != null) {
                    // 1. Î°úÍ∑∏Ïù∏ Ïã§Ìå®
                    if (rawMessage.contains("Bad credentials") ||
                            rawMessage.contains("password does not match") ||
                            rawMessage.contains("User not found") ||
                            rawMessage.contains("Authentication failed")) {

                        securityType = "Login Failure";
                        level = "WARN"; // Í≤©ÏÉÅ

                        // 2. Í∂åÌïú ÏóÜÏùå (Ìï¥ÌÇπ ÏãúÎèÑ ÏùòÏã¨)
                    } else if (rawMessage.contains("Access is denied") ||
                            rawMessage.contains("AccessDeniedException") ||
                            rawMessage.contains("AnonymouseAuthenticationToken")) {

                        securityType = "Access Denied";
                        level = "ERROR"; // Í≤©ÏÉÅ

                        // 3. CSRF Í≥µÍ≤©
                    } else if (rawMessage.contains("Invalid CSRF") ||
                            rawMessage.contains("Missing CSRF")) {

                        securityType = "CSRF Warning";
                        level = "ERROR"; // Í≤©ÏÉÅ

                        // 4. ÏÑ∏ÏÖò ÎßåÎ£å
                    } else if (rawMessage.contains("Session") && rawMessage.contains("expired")) {

                        securityType = "Session Expired";
                        level = "WARN";
                    }
                }
                // ---------------------------------------

                result.put("log_level", level);
                result.put("logger_name", loggerName);
                result.put("message", rawMessage);
                result.put("security_type", securityType); // ÌîÑÎ°†Ìä∏ÏóîÎìú ÌëúÏãúÏö© Ïú†Ìòï

                if (source.containsKey("tags")) {
                    result.put("tags", source.get("tags"));
                }
            }

        } else {
            // =================================================
            // [ETC] Í∏∞ÌÉÄ/Ïïå Ïàò ÏóÜÎäî Î°úÍ∑∏
            // =================================================
            result.putAll(source);
            result.put("log_level", source.getOrDefault("level", "INFO"));
            result.put("logger_name", source.getOrDefault("logger", "Unknown"));
            result.put("message", source.getOrDefault("message", "No message"));
        }

        return result;
    }

    /**
     * ÏóêÎü¨ ÏùëÎãµ ÏÉùÏÑ±
     * @param errorMessage ÏóêÎü¨ Î©îÏãúÏßÄ
     * @return Map
     */
    private Map<String, Object> createErrorResponse(String errorMessage) {
        Map<String, Object> errorResponse = new HashMap<>();
        errorResponse.put("total", 0);
        errorResponse.put("logs", Collections.emptyList());
        errorResponse.put("error", errorMessage);
        return errorResponse;
    }

    private Double getAggregationValue(SearchResponse<Void> response, String aggName) {
        try {
            if (response.aggregations() != null && response.aggregations().get(aggName) != null) {
                co.elastic.clients.elasticsearch._types.aggregations.Aggregate agg =
                        response.aggregations().get(aggName);

                if (agg.isAvg()) {
                    return agg.avg().value();
                } else if (agg.isMax()) {
                    return agg.max().value();
                } else if (agg.isSum()) {
                    return agg.sum().value();
                }
            }
        } catch (Exception e) {
            log.warn("Failed to get aggregation value: {}", aggName);
        }
        return 0.0;
    }

    private Double getBucketAggregationValue(
            co.elastic.clients.elasticsearch._types.aggregations.DateHistogramBucket bucket,
            String aggName) {
        try {
            if (bucket.aggregations() != null && bucket.aggregations().get(aggName) != null) {
                co.elastic.clients.elasticsearch._types.aggregations.Aggregate agg =
                        bucket.aggregations().get(aggName);

                if (agg.isAvg()) {
                    return agg.avg().value();
                }
            }
        } catch (Exception e) {
            log.warn("Failed to get bucket aggregation value: {}", aggName);
        }
        return 0.0;
    }
}